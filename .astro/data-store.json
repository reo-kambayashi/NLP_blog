[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.0","content-config-digest","6ab83d4c224a4955","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","papers",["Map",11,12,76,77],"large-language-models-are-human-level-prompt-engineers2023",{"id":11,"data":13,"body":16,"filePath":17,"digest":18,"rendered":19,"legacyId":75},{"title":14,"date":15},"Large Language Models Are Human-Level Prompt Engineers(2023)","2025-06-06","## 著者\n[Yongchao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+Y), [Andrei Ioan Muresanu](https://arxiv.org/search/cs?searchtype=author&query=Muresanu,+A+I), [Ziwen Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+Z), [Keiran Paster](https://arxiv.org/search/cs?searchtype=author&query=Paster,+K), [Silviu Pitis](https://arxiv.org/search/cs?searchtype=author&query=Pitis,+S), [Harris Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan,+H), [Jimmy Ba](https://arxiv.org/search/cs?searchtype=author&query=Ba,+J)\n\n## arXivリンク\nhttps://arxiv.org/abs/2211.01910\n\n## 要約\n- プロンプトエンジニアリングを自動化する「Automatic Prompt Engineer (APE)」を提案\n-  ざっくり**LLMにLLMのプロンプトを考えさせる**手法。\n- APEは人間が時間をかけて考えたものと同等かそれ以上の性能を達成\n-  APEのステップ\n\t- LLMに「こういう入力をしたらこういう出力をする」という例を見せて、指示文の候補をいっぱい作らせる\n\t- 候補を試し、その結果をスコアリング\n\t- 最も高いスコアの指示を最良のプロンプトとする\n\n## Abstract\n- LLMは汎用的でいいけど「人間が望むことをさせる」のって難しいよね\n\t- →プロンプト大事\n- 幅広いプロンプトを試す必要があるけどどの指示がどのモデルでいいのかわからない\n- LLMを自然言語でプログラムが指定されるブラックボックスなコンピュータとみなそう\n### 提案手法(APE)\nゼロショット学習において24件中24件のInstruction Inductionタスクと21件中17件のBig-Benchタスクで人間レベルの性能を達成\n\n## APEのアルゴリズム(お気持ちベース)\n### \"提案→評価とフィルタリング→選択\"のサイクルを自動化\n### 指示候補の提案\n- LLMにタスクの入出力例を見せ、タスクの遂行用指示文をいっぱい考えさせる。\n- 順方向生成\n\t- 例を先に提示し、最後にこういう指示でしたと続ける\n- 逆方向生成\n\t- \u003Cここに指示を挿入>的な感じで穴埋め問題にして、その後に例を提示\n### 評価・フィルタリング\n指示候補を使って、実際に別のLLMにタスクを解かせ、その性能をスコア付け\n#### 評価基準\n- **実行精度 (Execution accuracy)**: 指示通りにタスクを実行した結果が、想定される正解と一致したかどうかで評価\n- **対数尤度 (Log probability)**: どれだけ正解に近い答えを生成できそうかを確率で評価\n#### フィルタリング\n- 少数の訓練データで候補を評価\n- スコアが良かった上位数パーを残して破棄\n- 残ったものを別の訓練データで再評価\n- これを繰り返して少数の交互に絞る\n#### 選択\n- フィルタリングで残った候補から最も高いスコアのものを採用\n\n## APEを使った結果\n-  ゼロショットで24個のInstruction Inductionタスク全てにおいて、人間が作成したプロンプトと同等かそれ以上の性能を達成\n-  フューショットで24タスク中21タスクで性能が向上するか、同等の結果\n- 高難易度タスク(BIG-Bench)でも17/21で同等かそれ以上\n## いい感じの発見\n### Zero-shot Chain-of-Thought\n- \"Let's think step by step.\"をAPEが改善\n\t- \"Let's work this out in a step by step way to be sure we have the right answer\"のほうがいいらしい\n### TruthfulQA\n- APEがLLMの応答スタイルを制御して「真実性」と「情報提供性」のトレードオフを発見\n\t- 真実性をあげるために嘘をつかせないプロンプト(you have no comment→回答を拒否する選択肢をあたえる)をAPEが見つけた\n\n## 結論\n人間による入力を最小限にしつつ、最適なプロンプトをみつける方法としてAPEは有用\n\n## 感想\n- プロンプト集みたいなのがよくネットに転がってるけどこういうのでちゃんと性能が確認されているのか気になった。\n- 人間が直感的にわかりやすいプロンプトと、LLMがいい性能を示すプロンプトがちょっとちがっておもろい\n- モデル間でも最適なプロンプトがちがって、InstractGPTで最適なプロンプトをGPT-3で用いるとスコアが下がることがあるらしい←自分が使ってるモデルで最適なのを調べる必要あり\n- ↑モデルごと違うならそれこそ自動化とかしないと見つけるの大変そう","src/content/papers/Large Language Models Are Human-Level Prompt Engineers(2023).md","189ab1611cd5314f",{"html":20,"metadata":21},"\u003Ch2 id=\"著者\">著者\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhou,+Y\">Yongchao Zhou\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Muresanu,+A+I\">Andrei Ioan Muresanu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Han,+Z\">Ziwen Han\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Paster,+K\">Keiran Paster\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pitis,+S\">Silviu Pitis\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chan,+H\">Harris Chan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ba,+J\">Jimmy Ba\u003C/a>\u003C/p>\n\u003Ch2 id=\"arxivリンク\">arXivリンク\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/abs/2211.01910\">https://arxiv.org/abs/2211.01910\u003C/a>\u003C/p>\n\u003Ch2 id=\"要約\">要約\u003C/h2>\n\u003Cul>\n\u003Cli>プロンプトエンジニアリングを自動化する「Automatic Prompt Engineer (APE)」を提案\u003C/li>\n\u003Cli>ざっくり\u003Cstrong>LLMにLLMのプロンプトを考えさせる\u003C/strong>手法。\u003C/li>\n\u003Cli>APEは人間が時間をかけて考えたものと同等かそれ以上の性能を達成\u003C/li>\n\u003Cli>APEのステップ\n\u003Cul>\n\u003Cli>LLMに「こういう入力をしたらこういう出力をする」という例を見せて、指示文の候補をいっぱい作らせる\u003C/li>\n\u003Cli>候補を試し、その結果をスコアリング\u003C/li>\n\u003Cli>最も高いスコアの指示を最良のプロンプトとする\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"abstract\">Abstract\u003C/h2>\n\u003Cul>\n\u003Cli>LLMは汎用的でいいけど「人間が望むことをさせる」のって難しいよね\n\u003Cul>\n\u003Cli>→プロンプト大事\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>幅広いプロンプトを試す必要があるけどどの指示がどのモデルでいいのかわからない\u003C/li>\n\u003Cli>LLMを自然言語でプログラムが指定されるブラックボックスなコンピュータとみなそう\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"提案手法ape\">提案手法(APE)\u003C/h3>\n\u003Cp>ゼロショット学習において24件中24件のInstruction Inductionタスクと21件中17件のBig-Benchタスクで人間レベルの性能を達成\u003C/p>\n\u003Ch2 id=\"apeのアルゴリズムお気持ちベース\">APEのアルゴリズム(お気持ちベース)\u003C/h2>\n\u003Ch3 id=\"提案評価とフィルタリング選択のサイクルを自動化\">“提案→評価とフィルタリング→選択”のサイクルを自動化\u003C/h3>\n\u003Ch3 id=\"指示候補の提案\">指示候補の提案\u003C/h3>\n\u003Cul>\n\u003Cli>LLMにタスクの入出力例を見せ、タスクの遂行用指示文をいっぱい考えさせる。\u003C/li>\n\u003Cli>順方向生成\n\u003Cul>\n\u003Cli>例を先に提示し、最後にこういう指示でしたと続ける\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>逆方向生成\n\u003Cul>\n\u003Cli>&#x3C;ここに指示を挿入>的な感じで穴埋め問題にして、その後に例を提示\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"評価フィルタリング\">評価・フィルタリング\u003C/h3>\n\u003Cp>指示候補を使って、実際に別のLLMにタスクを解かせ、その性能をスコア付け\u003C/p>\n\u003Ch4 id=\"評価基準\">評価基準\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Cstrong>実行精度 (Execution accuracy)\u003C/strong>: 指示通りにタスクを実行した結果が、想定される正解と一致したかどうかで評価\u003C/li>\n\u003Cli>\u003Cstrong>対数尤度 (Log probability)\u003C/strong>: どれだけ正解に近い答えを生成できそうかを確率で評価\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"フィルタリング\">フィルタリング\u003C/h4>\n\u003Cul>\n\u003Cli>少数の訓練データで候補を評価\u003C/li>\n\u003Cli>スコアが良かった上位数パーを残して破棄\u003C/li>\n\u003Cli>残ったものを別の訓練データで再評価\u003C/li>\n\u003Cli>これを繰り返して少数の交互に絞る\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"選択\">選択\u003C/h4>\n\u003Cul>\n\u003Cli>フィルタリングで残った候補から最も高いスコアのものを採用\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"apeを使った結果\">APEを使った結果\u003C/h2>\n\u003Cul>\n\u003Cli>ゼロショットで24個のInstruction Inductionタスク全てにおいて、人間が作成したプロンプトと同等かそれ以上の性能を達成\u003C/li>\n\u003Cli>フューショットで24タスク中21タスクで性能が向上するか、同等の結果\u003C/li>\n\u003Cli>高難易度タスク(BIG-Bench)でも17/21で同等かそれ以上\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"いい感じの発見\">いい感じの発見\u003C/h2>\n\u003Ch3 id=\"zero-shot-chain-of-thought\">Zero-shot Chain-of-Thought\u003C/h3>\n\u003Cul>\n\u003Cli>“Let’s think step by step.”をAPEが改善\n\u003Cul>\n\u003Cli>“Let’s work this out in a step by step way to be sure we have the right answer”のほうがいいらしい\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"truthfulqa\">TruthfulQA\u003C/h3>\n\u003Cul>\n\u003Cli>APEがLLMの応答スタイルを制御して「真実性」と「情報提供性」のトレードオフを発見\n\u003Cul>\n\u003Cli>真実性をあげるために嘘をつかせないプロンプト(you have no comment→回答を拒否する選択肢をあたえる)をAPEが見つけた\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"結論\">結論\u003C/h2>\n\u003Cp>人間による入力を最小限にしつつ、最適なプロンプトをみつける方法としてAPEは有用\u003C/p>\n\u003Ch2 id=\"感想\">感想\u003C/h2>\n\u003Cul>\n\u003Cli>プロンプト集みたいなのがよくネットに転がってるけどこういうのでちゃんと性能が確認されているのか気になった。\u003C/li>\n\u003Cli>人間が直感的にわかりやすいプロンプトと、LLMがいい性能を示すプロンプトがちょっとちがっておもろい\u003C/li>\n\u003Cli>モデル間でも最適なプロンプトがちがって、InstractGPTで最適なプロンプトをGPT-3で用いるとスコアが下がることがあるらしい←自分が使ってるモデルで最適なのを調べる必要あり\u003C/li>\n\u003Cli>↑モデルごと違うならそれこそ自動化とかしないと見つけるの大変そう\u003C/li>\n\u003C/ul>",{"headings":22,"localImagePaths":71,"remoteImagePaths":72,"frontmatter":73,"imagePaths":74},[23,26,29,31,34,38,41,44,46,49,52,54,56,59,61,64,67,69],{"depth":24,"slug":25,"text":25},2,"著者",{"depth":24,"slug":27,"text":28},"arxivリンク","arXivリンク",{"depth":24,"slug":30,"text":30},"要約",{"depth":24,"slug":32,"text":33},"abstract","Abstract",{"depth":35,"slug":36,"text":37},3,"提案手法ape","提案手法(APE)",{"depth":24,"slug":39,"text":40},"apeのアルゴリズムお気持ちベース","APEのアルゴリズム(お気持ちベース)",{"depth":35,"slug":42,"text":43},"提案評価とフィルタリング選択のサイクルを自動化","“提案→評価とフィルタリング→選択”のサイクルを自動化",{"depth":35,"slug":45,"text":45},"指示候補の提案",{"depth":35,"slug":47,"text":48},"評価フィルタリング","評価・フィルタリング",{"depth":50,"slug":51,"text":51},4,"評価基準",{"depth":50,"slug":53,"text":53},"フィルタリング",{"depth":50,"slug":55,"text":55},"選択",{"depth":24,"slug":57,"text":58},"apeを使った結果","APEを使った結果",{"depth":24,"slug":60,"text":60},"いい感じの発見",{"depth":35,"slug":62,"text":63},"zero-shot-chain-of-thought","Zero-shot Chain-of-Thought",{"depth":35,"slug":65,"text":66},"truthfulqa","TruthfulQA",{"depth":24,"slug":68,"text":68},"結論",{"depth":24,"slug":70,"text":70},"感想",[],[],{"title":14,"date":15},[],"Large Language Models Are Human-Level Prompt Engineers(2023).md","the-llama-3-herd-of-models2024",{"id":76,"data":78,"body":81,"filePath":82,"digest":83,"rendered":84,"legacyId":120},{"title":79,"date":80},"The Llama 3 Herd of Models(2024)","2025-06-07","## arXiv\nhttps://arxiv.org/abs/2407.21783\n\n## 要約\n- Llama-3に関する研究開発\n- 15.6トークンで学習された8B,70B,405Bのdense Transformer\n- 128Kトークンのコンテクストウィンドウ\n- **DPO！！！**\n- 4oとかClaude3.5とかに匹敵するレベル\n- Llama-2 70BとLlama-3 8Bが同じぐらいの性能\n\n## 学習方法\n1. 教師なし学習により、大量のテキストコーパスから学習\n2. 現在のモデルでプリファレンスデータ（出力を人手で比較評価したやつ）を作成し、報酬モデリングを実施\n3. 現在のモデル＋報酬モデルSFT用のデータを作成し、ベースモデルをチューニング\n4. プリファレンスデータをLLMにマッチさせるようにDPOでベストモデルを更新\n\n## 開発における3つの主要要素\n### データ\n- 事前、事後両方の学習でのデータの質と量が向上\n- データミックスは、一般知識50％、数学的推論25％、コード17％、多言語トークン8％\n- 安全性のためのデータフィルタリング\n### スケール\n- Llama 2の最大verより約50倍の計算量でモデルを訓練\n### 複雑性の管理\n- 標準のdense Transformerのアーキテクチャを採用\n- MoEは複雑だから避けて安定性重視\n\t- MoEってなんですかの人はこちら\n\t- https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\n\n## Llama 2からの変更点\n### PPO → DPO\n- ここで出てきますねDPO\n### データ量\n- 1.8兆tokens → 15兆tokens\n### モデルアーキテクチャの微変更\n- Grouped Query Attention(GQA)の採用\n\t- GQAのGemini解説: https://g.co/gemini/share/5a3c1c47f73d\n\t- 推論速度向上\n\t- デコード中のキャッシュサイズ削減\n- トークンボキャブラリの拡張\n\t- tiktokenトークナイザの100K+非英語言語用の28Kトークン\n\t- 英語データの圧縮率が3.17文字/token から3.94文字/token に向上\n\t\t- 同じ計算量でいっぱい読める\n\t- RoPEベースの周波数ハイパラが増加\n\t\t- RoPEは位置情報を位置の加算じゃなくて回転するやつ\n\t\t- 500,000に増加(何が嬉しいのかよくわからなかった)\n\n## 感想\n- もうDPOしか頭に入ってこなかった\n- RoPEのところの話があんまり理解できなかった\n- やっぱりLlamaは安全性にすごい気を使っていそう\n\n## 著者(多すぎ)\n[Aaron Grattafiori](https://arxiv.org/search/cs?searchtype=author&query=Grattafiori,+A), [Abhimanyu Dubey](https://arxiv.org/search/cs?searchtype=author&query=Dubey,+A), [Abhinav Jauhri](https://arxiv.org/search/cs?searchtype=author&query=Jauhri,+A), [Abhinav Pandey](https://arxiv.org/search/cs?searchtype=author&query=Pandey,+A), [Abhishek Kadian](https://arxiv.org/search/cs?searchtype=author&query=Kadian,+A), [Ahmad Al-Dahle](https://arxiv.org/search/cs?searchtype=author&query=Al-Dahle,+A), [Aiesha Letman](https://arxiv.org/search/cs?searchtype=author&query=Letman,+A), [Akhil Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur,+A), [Alan Schelten](https://arxiv.org/search/cs?searchtype=author&query=Schelten,+A), [Alex Vaughan](https://arxiv.org/search/cs?searchtype=author&query=Vaughan,+A), [Amy Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+A), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan,+A), [Anirudh Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal,+A), [Anthony Hartshorn](https://arxiv.org/search/cs?searchtype=author&query=Hartshorn,+A), [Aobo Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+A), [Archi Mitra](https://arxiv.org/search/cs?searchtype=author&query=Mitra,+A), [Archie Sravankumar](https://arxiv.org/search/cs?searchtype=author&query=Archie), [Artem Korenev](https://arxiv.org/search/cs?searchtype=author&query=Korenev,+A), [Arthur Hinsvark](https://arxiv.org/search/cs?searchtype=author&query=Hinsvark,+A), [Arun Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao,+A), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+A), [Aurelien Rodriguez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez,+A), [Austen Gregerson](https://arxiv.org/search/cs?searchtype=author&query=Gregerson,+A), [Ava Spataru](https://arxiv.org/search/cs?searchtype=author&query=Spataru,+A), [Baptiste Roziere](https://arxiv.org/search/cs?searchtype=author&query=Roziere,+B), [Bethany Biron](https://arxiv.org/search/cs?searchtype=author&query=Biron,+B), [Binh Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+B), [Bobbie Chern](https://arxiv.org/search/cs?searchtype=author&query=Chern,+B), [Charlotte Caucheteux](https://arxiv.org/search/cs?searchtype=author&query=Caucheteux,+C), [Chaya Nayak](https://arxiv.org/search/cs?searchtype=author&query=Nayak,+C), [Chloe Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi,+C), [Chris Marra](https://arxiv.org/search/cs?searchtype=author&query=Marra,+C), [Chris McConnell](https://arxiv.org/search/cs?searchtype=author&query=McConnell,+C), [Christian Keller](https://arxiv.org/search/cs?searchtype=author&query=Keller,+C), [Christophe Touret](https://arxiv.org/search/cs?searchtype=author&query=Touret,+C), [Chunyang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+C), [Corinne Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong,+C), [Cristian Canton Ferrer](https://arxiv.org/search/cs?searchtype=author&query=Ferrer,+C+C), [Cyrus Nikolaidis](https://arxiv.org/search/cs?searchtype=author&query=Nikolaidis,+C), [Damien Allonsius](https://arxiv.org/search/cs?searchtype=author&query=Allonsius,+D), [Daniel Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+D), [Danielle Pintz](https://arxiv.org/search/cs?searchtype=author&query=Pintz,+D), [Danny Livshits](https://arxiv.org/search/cs?searchtype=author&query=Livshits,+D), [Danny Wyatt](https://arxiv.org/search/cs?searchtype=author&query=Wyatt,+D), [David Esiobu](https://arxiv.org/search/cs?searchtype=author&query=Esiobu,+D), [Dhruv Choudhary](https://arxiv.org/search/cs?searchtype=author&query=Choudhary,+D), [Dhruv Mahajan](https://arxiv.org/search/cs?searchtype=author&query=Mahajan,+D), [Diego Garcia-Olano](https://arxiv.org/search/cs?searchtype=author&query=Garcia-Olano,+D), [Diego Perino](https://arxiv.org/search/cs?searchtype=author&query=Perino,+D), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes,+D), [Egor Lakomkin](https://arxiv.org/search/cs?searchtype=author&query=Lakomkin,+E), [Ehab AlBadawy](https://arxiv.org/search/cs?searchtype=author&query=AlBadawy,+E), [Elina Lobanova](https://arxiv.org/search/cs?searchtype=author&query=Lobanova,+E), [Emily Dinan](https://arxiv.org/search/cs?searchtype=author&query=Dinan,+E), [Eric Michael Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith,+E+M), [Filip Radenovic](https://arxiv.org/search/cs?searchtype=author&query=Radenovic,+F), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzm%C3%A1n,+F), [Frank Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+F), [Gabriel Synnaeve](https://arxiv.org/search/cs?searchtype=author&query=Synnaeve,+G), [Gabrielle Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+G), [Georgia Lewis Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson,+G+L), [Govind Thattai](https://arxiv.org/search/cs?searchtype=author&query=Thattai,+G), [Graeme Nail](https://arxiv.org/search/cs?searchtype=author&query=Nail,+G), [Gregoire Mialon](https://arxiv.org/search/cs?searchtype=author&query=Mialon,+G), [Guan Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang,+G), [Guillem Cucurell](https://arxiv.org/search/cs?searchtype=author&query=Cucurell,+G), [Hailey Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen,+H), [Hannah Korevaar](https://arxiv.org/search/cs?searchtype=author&query=Korevaar,+H), [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Hugo Touvron](https://arxiv.org/search/cs?searchtype=author&query=Touvron,+H), [Iliyan Zarov](https://arxiv.org/search/cs?searchtype=author&query=Zarov,+I), [Imanol Arrieta Ibarra](https://arxiv.org/search/cs?searchtype=author&query=Ibarra,+I+A), [Isabel Kloumann](https://arxiv.org/search/cs?searchtype=author&query=Kloumann,+I), [Ishan Misra](https://arxiv.org/search/cs?searchtype=author&query=Misra,+I), [Ivan Evtimov](https://arxiv.org/search/cs?searchtype=author&query=Evtimov,+I), [Jack Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Jade Copet](https://arxiv.org/search/cs?searchtype=author&query=Copet,+J), [Jaewon Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Jan Geffert](https://arxiv.org/search/cs?searchtype=author&query=Geffert,+J), [Jana Vranes](https://arxiv.org/search/cs?searchtype=author&query=Vranes,+J), [Jason Park](https://arxiv.org/search/cs?searchtype=author&query=Park,+J), [Jay Mahadeokar](https://arxiv.org/search/cs?searchtype=author&query=Mahadeokar,+J), [Jeet Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah,+J), [Jelmer van der Linde](https://arxiv.org/search/cs?searchtype=author&query=van+der+Linde,+J), [Jennifer Billock](https://arxiv.org/search/cs?searchtype=author&query=Billock,+J), [Jenny Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong,+J), [Jenya Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Jeremy Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu,+J), [Jianfeng Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi,+J), [Jianyu Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+J), [Jiawen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+J), [Jie Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J), [Jiecao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+J), [Joanna Bitton](https://arxiv.org/search/cs?searchtype=author&query=Bitton,+J), [Joe Spisak](https://arxiv.org/search/cs?searchtype=author&query=Spisak,+J), [Jongsoo Park](https://arxiv.org/search/cs?searchtype=author&query=Park,+J), [Joseph Rocca](https://arxiv.org/search/cs?searchtype=author&query=Rocca,+J), [Joshua Johnstun](https://arxiv.org/search/cs?searchtype=author&query=Johnstun,+J), [Joshua Saxe](https://arxiv.org/search/cs?searchtype=author&query=Saxe,+J), [Junteng Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+J) et al. (460 additional authors not shown)","src/content/papers/The Llama 3 Herd of Models(2024).md","fde70171f61f06ce",{"html":85,"metadata":86},"\u003Ch2 id=\"arxiv\">arXiv\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/abs/2407.21783\">https://arxiv.org/abs/2407.21783\u003C/a>\u003C/p>\n\u003Ch2 id=\"要約\">要約\u003C/h2>\n\u003Cul>\n\u003Cli>Llama-3に関する研究開発\u003C/li>\n\u003Cli>15.6トークンで学習された8B,70B,405Bのdense Transformer\u003C/li>\n\u003Cli>128Kトークンのコンテクストウィンドウ\u003C/li>\n\u003Cli>\u003Cstrong>DPO！！！\u003C/strong>\u003C/li>\n\u003Cli>4oとかClaude3.5とかに匹敵するレベル\u003C/li>\n\u003Cli>Llama-2 70BとLlama-3 8Bが同じぐらいの性能\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"学習方法\">学習方法\u003C/h2>\n\u003Col>\n\u003Cli>教師なし学習により、大量のテキストコーパスから学習\u003C/li>\n\u003Cli>現在のモデルでプリファレンスデータ（出力を人手で比較評価したやつ）を作成し、報酬モデリングを実施\u003C/li>\n\u003Cli>現在のモデル＋報酬モデルSFT用のデータを作成し、ベースモデルをチューニング\u003C/li>\n\u003Cli>プリファレンスデータをLLMにマッチさせるようにDPOでベストモデルを更新\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"開発における3つの主要要素\">開発における3つの主要要素\u003C/h2>\n\u003Ch3 id=\"データ\">データ\u003C/h3>\n\u003Cul>\n\u003Cli>事前、事後両方の学習でのデータの質と量が向上\u003C/li>\n\u003Cli>データミックスは、一般知識50％、数学的推論25％、コード17％、多言語トークン8％\u003C/li>\n\u003Cli>安全性のためのデータフィルタリング\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"スケール\">スケール\u003C/h3>\n\u003Cul>\n\u003Cli>Llama 2の最大verより約50倍の計算量でモデルを訓練\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"複雑性の管理\">複雑性の管理\u003C/h3>\n\u003Cul>\n\u003Cli>標準のdense Transformerのアーキテクチャを採用\u003C/li>\n\u003Cli>MoEは複雑だから避けて安定性重視\n\u003Cul>\n\u003Cli>MoEってなんですかの人はこちら\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\">https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\u003C/a>\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"llama-2からの変更点\">Llama 2からの変更点\u003C/h2>\n\u003Ch3 id=\"ppo--dpo\">PPO → DPO\u003C/h3>\n\u003Cul>\n\u003Cli>ここで出てきますねDPO\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"データ量\">データ量\u003C/h3>\n\u003Cul>\n\u003Cli>1.8兆tokens → 15兆tokens\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"モデルアーキテクチャの微変更\">モデルアーキテクチャの微変更\u003C/h3>\n\u003Cul>\n\u003Cli>Grouped Query Attention(GQA)の採用\n\u003Cul>\n\u003Cli>GQAのGemini解説: \u003Ca href=\"https://g.co/gemini/share/5a3c1c47f73d\">https://g.co/gemini/share/5a3c1c47f73d\u003C/a>\u003C/li>\n\u003Cli>推論速度向上\u003C/li>\n\u003Cli>デコード中のキャッシュサイズ削減\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>トークンボキャブラリの拡張\n\u003Cul>\n\u003Cli>tiktokenトークナイザの100K+非英語言語用の28Kトークン\u003C/li>\n\u003Cli>英語データの圧縮率が3.17文字/token から3.94文字/token に向上\n\u003Cul>\n\u003Cli>同じ計算量でいっぱい読める\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>RoPEベースの周波数ハイパラが増加\n\u003Cul>\n\u003Cli>RoPEは位置情報を位置の加算じゃなくて回転するやつ\u003C/li>\n\u003Cli>500,000に増加(何が嬉しいのかよくわからなかった)\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"感想\">感想\u003C/h2>\n\u003Cul>\n\u003Cli>もうDPOしか頭に入ってこなかった\u003C/li>\n\u003Cli>RoPEのところの話があんまり理解できなかった\u003C/li>\n\u003Cli>やっぱりLlamaは安全性にすごい気を使っていそう\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"著者多すぎ\">著者(多すぎ)\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Grattafiori,+A\">Aaron Grattafiori\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Dubey,+A\">Abhimanyu Dubey\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Jauhri,+A\">Abhinav Jauhri\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pandey,+A\">Abhinav Pandey\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Kadian,+A\">Abhishek Kadian\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Al-Dahle,+A\">Ahmad Al-Dahle\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Letman,+A\">Aiesha Letman\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mathur,+A\">Akhil Mathur\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Schelten,+A\">Alan Schelten\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Vaughan,+A\">Alex Vaughan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yang,+A\">Amy Yang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Fan,+A\">Angela Fan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Goyal,+A\">Anirudh Goyal\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hartshorn,+A\">Anthony Hartshorn\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yang,+A\">Aobo Yang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mitra,+A\">Archi Mitra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Archie\">Archie Sravankumar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Korenev,+A\">Artem Korenev\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hinsvark,+A\">Arthur Hinsvark\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rao,+A\">Arun Rao\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+A\">Aston Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rodriguez,+A\">Aurelien Rodriguez\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Gregerson,+A\">Austen Gregerson\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Spataru,+A\">Ava Spataru\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Roziere,+B\">Baptiste Roziere\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Biron,+B\">Bethany Biron\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Tang,+B\">Binh Tang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chern,+B\">Bobbie Chern\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Caucheteux,+C\">Charlotte Caucheteux\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nayak,+C\">Chaya Nayak\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Bi,+C\">Chloe Bi\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Marra,+C\">Chris Marra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=McConnell,+C\">Chris McConnell\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Keller,+C\">Christian Keller\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Touret,+C\">Christophe Touret\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wu,+C\">Chunyang Wu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wong,+C\">Corinne Wong\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ferrer,+C+C\">Cristian Canton Ferrer\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nikolaidis,+C\">Cyrus Nikolaidis\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Allonsius,+D\">Damien Allonsius\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Song,+D\">Daniel Song\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pintz,+D\">Danielle Pintz\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Livshits,+D\">Danny Livshits\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wyatt,+D\">Danny Wyatt\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Esiobu,+D\">David Esiobu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Choudhary,+D\">Dhruv Choudhary\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mahajan,+D\">Dhruv Mahajan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Garcia-Olano,+D\">Diego Garcia-Olano\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Perino,+D\">Diego Perino\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hupkes,+D\">Dieuwke Hupkes\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lakomkin,+E\">Egor Lakomkin\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=AlBadawy,+E\">Ehab AlBadawy\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lobanova,+E\">Elina Lobanova\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Dinan,+E\">Emily Dinan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Smith,+E+M\">Eric Michael Smith\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Radenovic,+F\">Filip Radenovic\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Guzm%C3%A1n,+F\">Francisco Guzmán\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+F\">Frank Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Synnaeve,+G\">Gabriel Synnaeve\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+G\">Gabrielle Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Anderson,+G+L\">Georgia Lewis Anderson\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Thattai,+G\">Govind Thattai\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nail,+G\">Graeme Nail\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mialon,+G\">Gregoire Mialon\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pang,+G\">Guan Pang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Cucurell,+G\">Guillem Cucurell\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nguyen,+H\">Hailey Nguyen\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Korevaar,+H\">Hannah Korevaar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Xu,+H\">Hu Xu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Touvron,+H\">Hugo Touvron\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zarov,+I\">Iliyan Zarov\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ibarra,+I+A\">Imanol Arrieta Ibarra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Kloumann,+I\">Isabel Kloumann\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Misra,+I\">Ishan Misra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Evtimov,+I\">Ivan Evtimov\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+J\">Jack Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Copet,+J\">Jade Copet\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+J\">Jaewon Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Geffert,+J\">Jan Geffert\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Vranes,+J\">Jana Vranes\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Park,+J\">Jason Park\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mahadeokar,+J\">Jay Mahadeokar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Shah,+J\">Jeet Shah\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=van+der+Linde,+J\">Jelmer van der Linde\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Billock,+J\">Jennifer Billock\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hong,+J\">Jenny Hong\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+J\">Jenya Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Fu,+J\">Jeremy Fu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chi,+J\">Jianfeng Chi\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Huang,+J\">Jianyu Huang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Liu,+J\">Jiawen Liu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wang,+J\">Jie Wang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yu,+J\">Jiecao Yu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Bitton,+J\">Joanna Bitton\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Spisak,+J\">Joe Spisak\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Park,+J\">Jongsoo Park\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rocca,+J\">Joseph Rocca\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Johnstun,+J\">Joshua Johnstun\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Saxe,+J\">Joshua Saxe\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Jia,+J\">Junteng Jia\u003C/a> et al. (460 additional authors not shown)\u003C/p>",{"headings":87,"localImagePaths":116,"remoteImagePaths":117,"frontmatter":118,"imagePaths":119},[88,91,92,94,96,98,100,102,105,108,110,112,113],{"depth":24,"slug":89,"text":90},"arxiv","arXiv",{"depth":24,"slug":30,"text":30},{"depth":24,"slug":93,"text":93},"学習方法",{"depth":24,"slug":95,"text":95},"開発における3つの主要要素",{"depth":35,"slug":97,"text":97},"データ",{"depth":35,"slug":99,"text":99},"スケール",{"depth":35,"slug":101,"text":101},"複雑性の管理",{"depth":24,"slug":103,"text":104},"llama-2からの変更点","Llama 2からの変更点",{"depth":35,"slug":106,"text":107},"ppo--dpo","PPO → DPO",{"depth":35,"slug":109,"text":109},"データ量",{"depth":35,"slug":111,"text":111},"モデルアーキテクチャの微変更",{"depth":24,"slug":70,"text":70},{"depth":24,"slug":114,"text":115},"著者多すぎ","著者(多すぎ)",[],[],{"title":79,"date":80},[],"The Llama 3 Herd of Models(2024).md"]