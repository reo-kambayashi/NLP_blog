[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.0","content-config-digest","6ab83d4c224a4955","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","papers",["Map",11,12,62,63,103,104,174,175],"gpt-4",{"id":11,"data":13,"body":16,"filePath":17,"digest":18,"rendered":19,"legacyId":61},{"title":14,"date":15},"GPT-4: 大規模言語モデルの新たな地平","2023-03-15","## 概要\n\nGPT-4は、OpenAIが開発した大規模なマルチモーダル言語モデルです。前世代のGPT-3.5を大幅に上回る性能を示し、特に以下の点で注目されています：\n\n- **マルチモーダル入力**: テキストと画像の両方を理解可能\n- **人間レベルの性能**: 多くの専門的ベンチマークで人間と同等の成績\n- **安全性の向上**: 有害コンテンツの生成を大幅に削減\n\n## 主な技術的特徴\n\n### アーキテクチャ\n- Transformerベースの大規模ニューラルネットワーク\n- マルチモーダル入力に対応した統合アーキテクチャ\n- 詳細なモデルサイズは非公開\n\n### 学習プロセス\n1. **事前学習**: 大規模なテキスト・画像データセットでの教師なし学習\n2. **教師あり学習**: 高品質な人間の回答例を用いた学習\n3. **RLHF**: 人間のフィードバックからの強化学習による最適化\n\n## 性能評価\n\n### 学術的ベンチマーク\n- **Bar Exam**: 上位10%の成績\n- **SAT Math**: 700/800点\n- **GRE Verbal**: 99th percentile\n\n### プログラミング能力\n- **HumanEval**: 67%の問題を正解\n- **Codex-S**: 複雑なプログラミングタスクで高い性能\n\n## 実用的応用\n\n### 教育分野\n- 個別指導システム\n- 教材作成支援\n- 学習評価ツール\n\n### クリエイティブ分野\n- コンテンツ生成\n- アイデア発想支援\n- 創作活動のサポート\n\n### ビジネス応用\n- カスタマーサポート\n- 文書作成支援\n- データ分析レポート生成\n\n## 安全性と制限事項\n\n### 改善点\n- 有害コンテンツ生成率の大幅削減\n- より適切な拒否回答の生成\n- バイアスの軽減\n\n### 残存する課題\n- **幻覚**: 事実でない情報の生成\n- **計算コスト**: 高い推論コスト\n- **知識カットオフ**: 学習データの時点的制限\n\n## 今後の展望\n\nGPT-4の登場により、AI研究は新たな段階に入りました。今後期待される発展：\n\n1. **効率化**: より軽量で高性能なモデルの開発\n2. **専門化**: 特定ドメインに特化したモデル\n3. **統合**: 他のAIシステムとの連携強化\n4. **民主化**: より多くの人々がアクセス可能な形での提供\n\n## 個人的感想\n\nGPT-4は単なる技術的進歩を超えて、人間とAIの関係性を根本的に変える可能性を秘めています。特にマルチモーダル機能は、これまで不可能だった新しい形のAI活用を可能にしており、今後数年間でAI応用の幅が大きく広がることが予想されます。\n\n研究者として注目すべきは、性能向上と安全性確保の両立に成功している点です。これは今後のAI開発における重要な指針を示しているといえるでしょう。","src/content/papers/gpt-4.md","4e3c519c02e67f20",{"html":20,"metadata":21},"\u003Ch2 id=\"概要\">概要\u003C/h2>\n\u003Cp>GPT-4は、OpenAIが開発した大規模なマルチモーダル言語モデルです。前世代のGPT-3.5を大幅に上回る性能を示し、特に以下の点で注目されています：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>マルチモーダル入力\u003C/strong>: テキストと画像の両方を理解可能\u003C/li>\n\u003Cli>\u003Cstrong>人間レベルの性能\u003C/strong>: 多くの専門的ベンチマークで人間と同等の成績\u003C/li>\n\u003Cli>\u003Cstrong>安全性の向上\u003C/strong>: 有害コンテンツの生成を大幅に削減\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"主な技術的特徴\">主な技術的特徴\u003C/h2>\n\u003Ch3 id=\"アーキテクチャ\">アーキテクチャ\u003C/h3>\n\u003Cul>\n\u003Cli>Transformerベースの大規模ニューラルネットワーク\u003C/li>\n\u003Cli>マルチモーダル入力に対応した統合アーキテクチャ\u003C/li>\n\u003Cli>詳細なモデルサイズは非公開\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"学習プロセス\">学習プロセス\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>事前学習\u003C/strong>: 大規模なテキスト・画像データセットでの教師なし学習\u003C/li>\n\u003Cli>\u003Cstrong>教師あり学習\u003C/strong>: 高品質な人間の回答例を用いた学習\u003C/li>\n\u003Cli>\u003Cstrong>RLHF\u003C/strong>: 人間のフィードバックからの強化学習による最適化\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"性能評価\">性能評価\u003C/h2>\n\u003Ch3 id=\"学術的ベンチマーク\">学術的ベンチマーク\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Bar Exam\u003C/strong>: 上位10%の成績\u003C/li>\n\u003Cli>\u003Cstrong>SAT Math\u003C/strong>: 700/800点\u003C/li>\n\u003Cli>\u003Cstrong>GRE Verbal\u003C/strong>: 99th percentile\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"プログラミング能力\">プログラミング能力\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>HumanEval\u003C/strong>: 67%の問題を正解\u003C/li>\n\u003Cli>\u003Cstrong>Codex-S\u003C/strong>: 複雑なプログラミングタスクで高い性能\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"実用的応用\">実用的応用\u003C/h2>\n\u003Ch3 id=\"教育分野\">教育分野\u003C/h3>\n\u003Cul>\n\u003Cli>個別指導システム\u003C/li>\n\u003Cli>教材作成支援\u003C/li>\n\u003Cli>学習評価ツール\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"クリエイティブ分野\">クリエイティブ分野\u003C/h3>\n\u003Cul>\n\u003Cli>コンテンツ生成\u003C/li>\n\u003Cli>アイデア発想支援\u003C/li>\n\u003Cli>創作活動のサポート\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"ビジネス応用\">ビジネス応用\u003C/h3>\n\u003Cul>\n\u003Cli>カスタマーサポート\u003C/li>\n\u003Cli>文書作成支援\u003C/li>\n\u003Cli>データ分析レポート生成\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"安全性と制限事項\">安全性と制限事項\u003C/h2>\n\u003Ch3 id=\"改善点\">改善点\u003C/h3>\n\u003Cul>\n\u003Cli>有害コンテンツ生成率の大幅削減\u003C/li>\n\u003Cli>より適切な拒否回答の生成\u003C/li>\n\u003Cli>バイアスの軽減\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"残存する課題\">残存する課題\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>幻覚\u003C/strong>: 事実でない情報の生成\u003C/li>\n\u003Cli>\u003Cstrong>計算コスト\u003C/strong>: 高い推論コスト\u003C/li>\n\u003Cli>\u003Cstrong>知識カットオフ\u003C/strong>: 学習データの時点的制限\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"今後の展望\">今後の展望\u003C/h2>\n\u003Cp>GPT-4の登場により、AI研究は新たな段階に入りました。今後期待される発展：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>効率化\u003C/strong>: より軽量で高性能なモデルの開発\u003C/li>\n\u003Cli>\u003Cstrong>専門化\u003C/strong>: 特定ドメインに特化したモデル\u003C/li>\n\u003Cli>\u003Cstrong>統合\u003C/strong>: 他のAIシステムとの連携強化\u003C/li>\n\u003Cli>\u003Cstrong>民主化\u003C/strong>: より多くの人々がアクセス可能な形での提供\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"個人的感想\">個人的感想\u003C/h2>\n\u003Cp>GPT-4は単なる技術的進歩を超えて、人間とAIの関係性を根本的に変える可能性を秘めています。特にマルチモーダル機能は、これまで不可能だった新しい形のAI活用を可能にしており、今後数年間でAI応用の幅が大きく広がることが予想されます。\u003C/p>\n\u003Cp>研究者として注目すべきは、性能向上と安全性確保の両立に成功している点です。これは今後のAI開発における重要な指針を示しているといえるでしょう。\u003C/p>",{"headings":22,"localImagePaths":57,"remoteImagePaths":58,"frontmatter":59,"imagePaths":60},[23,26,28,31,33,35,37,39,41,43,45,47,49,51,53,55],{"depth":24,"slug":25,"text":25},2,"概要",{"depth":24,"slug":27,"text":27},"主な技術的特徴",{"depth":29,"slug":30,"text":30},3,"アーキテクチャ",{"depth":29,"slug":32,"text":32},"学習プロセス",{"depth":24,"slug":34,"text":34},"性能評価",{"depth":29,"slug":36,"text":36},"学術的ベンチマーク",{"depth":29,"slug":38,"text":38},"プログラミング能力",{"depth":24,"slug":40,"text":40},"実用的応用",{"depth":29,"slug":42,"text":42},"教育分野",{"depth":29,"slug":44,"text":44},"クリエイティブ分野",{"depth":29,"slug":46,"text":46},"ビジネス応用",{"depth":24,"slug":48,"text":48},"安全性と制限事項",{"depth":29,"slug":50,"text":50},"改善点",{"depth":29,"slug":52,"text":52},"残存する課題",{"depth":24,"slug":54,"text":54},"今後の展望",{"depth":24,"slug":56,"text":56},"個人的感想",[],[],{"title":14,"date":15},[],"gpt-4.md","attention-is-all-you-need",{"id":62,"data":64,"body":67,"filePath":68,"digest":69,"rendered":70,"legacyId":102},{"title":65,"date":66},"Attention Is All You Need","2024-01-15","## 背景\n\nRecurrent Neural Networks (RNN) や Convolutional Neural Networks (CNN) はsequence modelingにおいて主流でしたが、以下の課題がありました：\n\n- **並列化の困難**: RNNは逐次処理のため並列化が困難\n- **長距離依存関係**: 長いsequenceでの情報伝達が困難\n- **計算効率**: 学習・推論に時間がかかる\n\n## 提案手法\n\n### Transformer アーキテクチャ\n\n本論文では、**attention機構のみ**を使用した新しいアーキテクチャ「Transformer」を提案しました。\n\n#### 主要コンポーネント\n\n1. **Multi-Head Attention**\n   - 複数の注意ヘッドで異なる部分空間の情報を捉える\n   - Self-attentionでsequence内の全ての位置間の関係を直接モデル化\n\n2. **Position Encoding**\n   - Sinusoidal関数を使用してposition情報を埋め込み\n   - RNNなしでsequenceの順序情報を保持\n\n3. **Feed-Forward Networks**\n   - 各位置に独立して適用される全結合層\n   - ReLU活性化関数を使用\n\n### 数式\n\nSelf-Attentionの計算式：\n\n```\nAttention(Q, K, V) = softmax(QK^T / √d_k)V\n```\n\nMulti-Head Attentionの計算式：\n\n```\nMultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\nwhere head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n```\n\n## 実験結果\n\n### 機械翻訳タスク\n\n- **WMT 2014 English-to-German**: BLEU score 28.4 (従来手法より+2.0)\n- **WMT 2014 English-to-French**: BLEU score 41.8 (新記録)\n\n### 学習効率\n\n- **学習時間**: 従来モデルの1/3以下\n- **パラメータ数**: 効率的な設計で少ないパラメータで高性能\n\n## 影響と応用\n\nこの論文以降、Transformerは様々な分野で応用されています：\n\n- **BERT**: 双方向Transformer for言語理解\n- **GPT**: 生成的Transformer for言語生成\n- **Vision Transformer**: 画像認識への応用\n- **CLIP**: マルチモーダルな表現学習\n\n## 個人的な考察\n\nこの論文は機械学習史上最も影響力のある論文の一つです。特に以下の点が革命的でした：\n\n1. **パラダイムシフト**: RNN/CNNからattention-onlyへ\n2. **スケーラビリティ**: 大規模並列処理が可能\n3. **汎用性**: NLP以外の分野にも応用可能\n\n現在のLarge Language Models (LLMs) の基礎となっており、ChatGPTやGPT-4などの成功もこの論文なしには考えられません。\n\n## 今後の研究課題\n\n- **効率性の改善**: O(n²)の計算量を削減する手法\n- **長文書処理**: より長いsequenceを効率的に処理\n- **解釈性**: attention patternの意味理解","src/content/papers/attention-is-all-you-need.md","5ac651f1aaef36f2",{"html":71,"metadata":72},"\u003Ch2 id=\"背景\">背景\u003C/h2>\n\u003Cp>Recurrent Neural Networks (RNN) や Convolutional Neural Networks (CNN) はsequence modelingにおいて主流でしたが、以下の課題がありました：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>並列化の困難\u003C/strong>: RNNは逐次処理のため並列化が困難\u003C/li>\n\u003Cli>\u003Cstrong>長距離依存関係\u003C/strong>: 長いsequenceでの情報伝達が困難\u003C/li>\n\u003Cli>\u003Cstrong>計算効率\u003C/strong>: 学習・推論に時間がかかる\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"提案手法\">提案手法\u003C/h2>\n\u003Ch3 id=\"transformer-アーキテクチャ\">Transformer アーキテクチャ\u003C/h3>\n\u003Cp>本論文では、\u003Cstrong>attention機構のみ\u003C/strong>を使用した新しいアーキテクチャ「Transformer」を提案しました。\u003C/p>\n\u003Ch4 id=\"主要コンポーネント\">主要コンポーネント\u003C/h4>\n\u003Col>\n\u003Cli>\n\u003Cp>\u003Cstrong>Multi-Head Attention\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>複数の注意ヘッドで異なる部分空間の情報を捉える\u003C/li>\n\u003Cli>Self-attentionでsequence内の全ての位置間の関係を直接モデル化\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Position Encoding\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>Sinusoidal関数を使用してposition情報を埋め込み\u003C/li>\n\u003Cli>RNNなしでsequenceの順序情報を保持\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\n\u003Cp>\u003Cstrong>Feed-Forward Networks\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>各位置に独立して適用される全結合層\u003C/li>\n\u003Cli>ReLU活性化関数を使用\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"数式\">数式\u003C/h3>\n\u003Cp>Self-Attentionの計算式：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Attention(Q, K, V) = softmax(QK^T / √d_k)V\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Multi-Head Attentionの計算式：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"実験結果\">実験結果\u003C/h2>\n\u003Ch3 id=\"機械翻訳タスク\">機械翻訳タスク\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>WMT 2014 English-to-German\u003C/strong>: BLEU score 28.4 (従来手法より+2.0)\u003C/li>\n\u003Cli>\u003Cstrong>WMT 2014 English-to-French\u003C/strong>: BLEU score 41.8 (新記録)\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"学習効率\">学習効率\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>学習時間\u003C/strong>: 従来モデルの1/3以下\u003C/li>\n\u003Cli>\u003Cstrong>パラメータ数\u003C/strong>: 効率的な設計で少ないパラメータで高性能\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"影響と応用\">影響と応用\u003C/h2>\n\u003Cp>この論文以降、Transformerは様々な分野で応用されています：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>BERT\u003C/strong>: 双方向Transformer for言語理解\u003C/li>\n\u003Cli>\u003Cstrong>GPT\u003C/strong>: 生成的Transformer for言語生成\u003C/li>\n\u003Cli>\u003Cstrong>Vision Transformer\u003C/strong>: 画像認識への応用\u003C/li>\n\u003Cli>\u003Cstrong>CLIP\u003C/strong>: マルチモーダルな表現学習\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"個人的な考察\">個人的な考察\u003C/h2>\n\u003Cp>この論文は機械学習史上最も影響力のある論文の一つです。特に以下の点が革命的でした：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>パラダイムシフト\u003C/strong>: RNN/CNNからattention-onlyへ\u003C/li>\n\u003Cli>\u003Cstrong>スケーラビリティ\u003C/strong>: 大規模並列処理が可能\u003C/li>\n\u003Cli>\u003Cstrong>汎用性\u003C/strong>: NLP以外の分野にも応用可能\u003C/li>\n\u003C/ol>\n\u003Cp>現在のLarge Language Models (LLMs) の基礎となっており、ChatGPTやGPT-4などの成功もこの論文なしには考えられません。\u003C/p>\n\u003Ch2 id=\"今後の研究課題\">今後の研究課題\u003C/h2>\n\u003Cul>\n\u003Cli>\u003Cstrong>効率性の改善\u003C/strong>: O(n²)の計算量を削減する手法\u003C/li>\n\u003Cli>\u003Cstrong>長文書処理\u003C/strong>: より長いsequenceを効率的に処理\u003C/li>\n\u003Cli>\u003Cstrong>解釈性\u003C/strong>: attention patternの意味理解\u003C/li>\n\u003C/ul>",{"headings":73,"localImagePaths":98,"remoteImagePaths":99,"frontmatter":100,"imagePaths":101},[74,76,78,81,84,86,88,90,92,94,96],{"depth":24,"slug":75,"text":75},"背景",{"depth":24,"slug":77,"text":77},"提案手法",{"depth":29,"slug":79,"text":80},"transformer-アーキテクチャ","Transformer アーキテクチャ",{"depth":82,"slug":83,"text":83},4,"主要コンポーネント",{"depth":29,"slug":85,"text":85},"数式",{"depth":24,"slug":87,"text":87},"実験結果",{"depth":29,"slug":89,"text":89},"機械翻訳タスク",{"depth":29,"slug":91,"text":91},"学習効率",{"depth":24,"slug":93,"text":93},"影響と応用",{"depth":24,"slug":95,"text":95},"個人的な考察",{"depth":24,"slug":97,"text":97},"今後の研究課題",[],[],{"title":65,"date":66},[],"attention-is-all-you-need.md","bert",{"id":103,"data":105,"body":108,"filePath":109,"digest":110,"rendered":111,"legacyId":173},{"title":106,"date":107},"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","2024-01-20","## 導入\n\nBERT (Bidirectional Encoder Representations from Transformers) は、Google AI Language チームによって開発された画期的な言語モデルです。従来の言語モデルが一方向（左から右、または右から左）の文脈しか見ないのに対し、BERTは**双方向**の文脈を同時に考慮できる点が革新的でした。\n\n## 背景と動機\n\n### 従来手法の限界\n\n1. **ELMo**: 双方向だが、left-to-right と right-to-left を独立に学習\n2. **GPT**: 強力だが一方向のみ（left-to-right）\n3. **従来の転移学習**: タスク固有のアーキテクチャが必要\n\n### BERTの目標\n\n- **双方向文脈**: 両方向の情報を同時に活用\n- **タスク非依存**: 様々なNLPタスクに適用可能\n- **ファインチューニング**: 少ない追加パラメータで高性能\n\n## 手法\n\n### 1. 事前学習タスク\n\n#### Masked Language Model (MLM)\n- 入力の15%のトークンをマスク\n- マスクされたトークンを予測\n- 双方向の文脈を学習可能\n\n```\nInput:  [CLS] my dog is [MASK] [SEP]\nTarget: [CLS] my dog is cute [SEP]\n```\n\n#### Next Sentence Prediction (NSP)\n- 二つの文が連続しているかを予測\n- 文レベルの関係性を学習\n\n```\nInput:  [CLS] The man went to the store. [SEP] He bought milk. [SEP]\nLabel:  IsNext\n\nInput:  [CLS] The man went to the store. [SEP] Penguins are flightless. [SEP]\nLabel:  NotNext\n```\n\n### 2. アーキテクチャ\n\n- **Base**: L=12, H=768, A=12, Total Parameters=110M\n- **Large**: L=24, H=1024, A=16, Total Parameters=340M\n\nWhere:\n- L = Transformer layers数\n- H = Hidden size\n- A = Attention heads数\n\n### 3. ファインチューニング\n\n各タスクに応じて出力層を追加し、エンドツーエンドでファインチューニング：\n\n- **分類タスク**: [CLS]トークンの表現を使用\n- **系列ラベリング**: 各トークンの表現を使用\n- **質問応答**: スパン予測のための出力層\n\n## 実験結果\n\n### GLUE Benchmark\n\n| Task | Metric | BERT-Base | BERT-Large | Previous SOTA |\n|------|--------|-----------|------------|---------------|\n| MNLI | Accuracy | 84.6 | 86.7 | 86.3 |\n| QQP | Accuracy | 89.2 | 89.3 | 86.1 |\n| QNLI | Accuracy | 90.5 | 92.7 | 87.4 |\n| SST-2 | Accuracy | 93.5 | 94.9 | 95.8 |\n\n### SQuAD 1.1\n\n- **BERT-Large**: F1 93.2, EM 87.4\n- **Previous SOTA**: F1 91.8, EM 85.8\n\n### SQuAD 2.0\n\n- **BERT-Large**: F1 83.1, EM 80.0\n- **Previous SOTA**: F1 76.3, EM 73.7\n\n## アブレーション研究\n\n### NSPの効果\n- NSPありのBERT: 84.4% (MNLI)\n- NSPなしのBERT: 84.0% (MNLI)\n- → NSPは小さいが一定の効果\n\n### 双方向性の重要性\n- 双方向BERT: 84.4% (MNLI)\n- Left-to-Right: 81.4% (MNLI)\n- Left-to-Right + Right-to-Left: 82.1% (MNLI)\n\n### モデルサイズの影響\n- より大きなモデルほど性能向上\n- 事前学習データが豊富なら、大きなモデルが有効\n\n## 影響と応用\n\nBERTの登場により、NLP分野は大きく変化しました：\n\n### 後続モデル\n- **RoBERTa**: BERTの改良版、NSPを除去\n- **ALBERT**: パラメータ共有による軽量化\n- **DistilBERT**: 知識蒸留による高速化\n- **ELECTRA**: より効率的な事前学習\n\n### 応用分野\n- **検索エンジン**: Google検索でBERTを活用\n- **チャットボット**: 文脈理解の向上\n- **翻訳**: より自然な翻訳\n- **要約**: 文書要約の精度向上\n\n## 個人的な感想\n\nBERTは「事前学習 + ファインチューニング」のパラダイムを確立し、現在のLLMブームの基礎を築きました。特に印象的なのは：\n\n1. **シンプルさ**: 複雑なタスク固有アーキテクチャが不要\n2. **汎用性**: 一つのモデルで多様なタスクに対応\n3. **性能**: 大幅な性能向上を実現\n\nただし、計算コストの高さや、より大きなモデルでのスケーリング特性については、当時は十分に議論されていませんでした。\n\n## 今後の課題\n\nBERTから学んだ教訓と今後の方向性：\n\n- **効率性**: より少ない計算で同等の性能\n- **長文書処理**: 512トークンの制限を超える処理\n- **マルチモーダル**: テキスト以外のデータとの統合\n- **解釈性**: モデルの判断根拠の理解","src/content/papers/bert.md","ae7716595de5a73d",{"html":112,"metadata":113},"\u003Ch2 id=\"導入\">導入\u003C/h2>\n\u003Cp>BERT (Bidirectional Encoder Representations from Transformers) は、Google AI Language チームによって開発された画期的な言語モデルです。従来の言語モデルが一方向（左から右、または右から左）の文脈しか見ないのに対し、BERTは\u003Cstrong>双方向\u003C/strong>の文脈を同時に考慮できる点が革新的でした。\u003C/p>\n\u003Ch2 id=\"背景と動機\">背景と動機\u003C/h2>\n\u003Ch3 id=\"従来手法の限界\">従来手法の限界\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>ELMo\u003C/strong>: 双方向だが、left-to-right と right-to-left を独立に学習\u003C/li>\n\u003Cli>\u003Cstrong>GPT\u003C/strong>: 強力だが一方向のみ（left-to-right）\u003C/li>\n\u003Cli>\u003Cstrong>従来の転移学習\u003C/strong>: タスク固有のアーキテクチャが必要\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"bertの目標\">BERTの目標\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>双方向文脈\u003C/strong>: 両方向の情報を同時に活用\u003C/li>\n\u003Cli>\u003Cstrong>タスク非依存\u003C/strong>: 様々なNLPタスクに適用可能\u003C/li>\n\u003Cli>\u003Cstrong>ファインチューニング\u003C/strong>: 少ない追加パラメータで高性能\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"手法\">手法\u003C/h2>\n\u003Ch3 id=\"1-事前学習タスク\">1. 事前学習タスク\u003C/h3>\n\u003Ch4 id=\"masked-language-model-mlm\">Masked Language Model (MLM)\u003C/h4>\n\u003Cul>\n\u003Cli>入力の15%のトークンをマスク\u003C/li>\n\u003Cli>マスクされたトークンを予測\u003C/li>\n\u003Cli>双方向の文脈を学習可能\u003C/li>\n\u003C/ul>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Input:  [CLS] my dog is [MASK] [SEP]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Target: [CLS] my dog is cute [SEP]\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch4 id=\"next-sentence-prediction-nsp\">Next Sentence Prediction (NSP)\u003C/h4>\n\u003Cul>\n\u003Cli>二つの文が連続しているかを予測\u003C/li>\n\u003Cli>文レベルの関係性を学習\u003C/li>\n\u003C/ul>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>Input:  [CLS] The man went to the store. [SEP] He bought milk. [SEP]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Label:  IsNext\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Input:  [CLS] The man went to the store. [SEP] Penguins are flightless. [SEP]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan>Label:  NotNext\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch3 id=\"2-アーキテクチャ\">2. アーキテクチャ\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>Base\u003C/strong>: L=12, H=768, A=12, Total Parameters=110M\u003C/li>\n\u003Cli>\u003Cstrong>Large\u003C/strong>: L=24, H=1024, A=16, Total Parameters=340M\u003C/li>\n\u003C/ul>\n\u003Cp>Where:\u003C/p>\n\u003Cul>\n\u003Cli>L = Transformer layers数\u003C/li>\n\u003Cli>H = Hidden size\u003C/li>\n\u003Cli>A = Attention heads数\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"3-ファインチューニング\">3. ファインチューニング\u003C/h3>\n\u003Cp>各タスクに応じて出力層を追加し、エンドツーエンドでファインチューニング：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>分類タスク\u003C/strong>: [CLS]トークンの表現を使用\u003C/li>\n\u003Cli>\u003Cstrong>系列ラベリング\u003C/strong>: 各トークンの表現を使用\u003C/li>\n\u003Cli>\u003Cstrong>質問応答\u003C/strong>: スパン予測のための出力層\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"実験結果\">実験結果\u003C/h2>\n\u003Ch3 id=\"glue-benchmark\">GLUE Benchmark\u003C/h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Task\u003C/th>\u003Cth>Metric\u003C/th>\u003Cth>BERT-Base\u003C/th>\u003Cth>BERT-Large\u003C/th>\u003Cth>Previous SOTA\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>MNLI\u003C/td>\u003Ctd>Accuracy\u003C/td>\u003Ctd>84.6\u003C/td>\u003Ctd>86.7\u003C/td>\u003Ctd>86.3\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>QQP\u003C/td>\u003Ctd>Accuracy\u003C/td>\u003Ctd>89.2\u003C/td>\u003Ctd>89.3\u003C/td>\u003Ctd>86.1\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>QNLI\u003C/td>\u003Ctd>Accuracy\u003C/td>\u003Ctd>90.5\u003C/td>\u003Ctd>92.7\u003C/td>\u003Ctd>87.4\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>SST-2\u003C/td>\u003Ctd>Accuracy\u003C/td>\u003Ctd>93.5\u003C/td>\u003Ctd>94.9\u003C/td>\u003Ctd>95.8\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch3 id=\"squad-11\">SQuAD 1.1\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>BERT-Large\u003C/strong>: F1 93.2, EM 87.4\u003C/li>\n\u003Cli>\u003Cstrong>Previous SOTA\u003C/strong>: F1 91.8, EM 85.8\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"squad-20\">SQuAD 2.0\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>BERT-Large\u003C/strong>: F1 83.1, EM 80.0\u003C/li>\n\u003Cli>\u003Cstrong>Previous SOTA\u003C/strong>: F1 76.3, EM 73.7\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"アブレーション研究\">アブレーション研究\u003C/h2>\n\u003Ch3 id=\"nspの効果\">NSPの効果\u003C/h3>\n\u003Cul>\n\u003Cli>NSPありのBERT: 84.4% (MNLI)\u003C/li>\n\u003Cli>NSPなしのBERT: 84.0% (MNLI)\u003C/li>\n\u003Cli>→ NSPは小さいが一定の効果\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"双方向性の重要性\">双方向性の重要性\u003C/h3>\n\u003Cul>\n\u003Cli>双方向BERT: 84.4% (MNLI)\u003C/li>\n\u003Cli>Left-to-Right: 81.4% (MNLI)\u003C/li>\n\u003Cli>Left-to-Right + Right-to-Left: 82.1% (MNLI)\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"モデルサイズの影響\">モデルサイズの影響\u003C/h3>\n\u003Cul>\n\u003Cli>より大きなモデルほど性能向上\u003C/li>\n\u003Cli>事前学習データが豊富なら、大きなモデルが有効\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"影響と応用\">影響と応用\u003C/h2>\n\u003Cp>BERTの登場により、NLP分野は大きく変化しました：\u003C/p>\n\u003Ch3 id=\"後続モデル\">後続モデル\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>RoBERTa\u003C/strong>: BERTの改良版、NSPを除去\u003C/li>\n\u003Cli>\u003Cstrong>ALBERT\u003C/strong>: パラメータ共有による軽量化\u003C/li>\n\u003Cli>\u003Cstrong>DistilBERT\u003C/strong>: 知識蒸留による高速化\u003C/li>\n\u003Cli>\u003Cstrong>ELECTRA\u003C/strong>: より効率的な事前学習\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"応用分野\">応用分野\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>検索エンジン\u003C/strong>: Google検索でBERTを活用\u003C/li>\n\u003Cli>\u003Cstrong>チャットボット\u003C/strong>: 文脈理解の向上\u003C/li>\n\u003Cli>\u003Cstrong>翻訳\u003C/strong>: より自然な翻訳\u003C/li>\n\u003Cli>\u003Cstrong>要約\u003C/strong>: 文書要約の精度向上\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"個人的な感想\">個人的な感想\u003C/h2>\n\u003Cp>BERTは「事前学習 + ファインチューニング」のパラダイムを確立し、現在のLLMブームの基礎を築きました。特に印象的なのは：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>シンプルさ\u003C/strong>: 複雑なタスク固有アーキテクチャが不要\u003C/li>\n\u003Cli>\u003Cstrong>汎用性\u003C/strong>: 一つのモデルで多様なタスクに対応\u003C/li>\n\u003Cli>\u003Cstrong>性能\u003C/strong>: 大幅な性能向上を実現\u003C/li>\n\u003C/ol>\n\u003Cp>ただし、計算コストの高さや、より大きなモデルでのスケーリング特性については、当時は十分に議論されていませんでした。\u003C/p>\n\u003Ch2 id=\"今後の課題\">今後の課題\u003C/h2>\n\u003Cp>BERTから学んだ教訓と今後の方向性：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>効率性\u003C/strong>: より少ない計算で同等の性能\u003C/li>\n\u003Cli>\u003Cstrong>長文書処理\u003C/strong>: 512トークンの制限を超える処理\u003C/li>\n\u003Cli>\u003Cstrong>マルチモーダル\u003C/strong>: テキスト以外のデータとの統合\u003C/li>\n\u003Cli>\u003Cstrong>解釈性\u003C/strong>: モデルの判断根拠の理解\u003C/li>\n\u003C/ul>",{"headings":114,"localImagePaths":169,"remoteImagePaths":170,"frontmatter":171,"imagePaths":172},[115,117,119,121,124,126,129,132,135,138,141,142,145,148,151,153,156,158,160,161,163,165,167],{"depth":24,"slug":116,"text":116},"導入",{"depth":24,"slug":118,"text":118},"背景と動機",{"depth":29,"slug":120,"text":120},"従来手法の限界",{"depth":29,"slug":122,"text":123},"bertの目標","BERTの目標",{"depth":24,"slug":125,"text":125},"手法",{"depth":29,"slug":127,"text":128},"1-事前学習タスク","1. 事前学習タスク",{"depth":82,"slug":130,"text":131},"masked-language-model-mlm","Masked Language Model (MLM)",{"depth":82,"slug":133,"text":134},"next-sentence-prediction-nsp","Next Sentence Prediction (NSP)",{"depth":29,"slug":136,"text":137},"2-アーキテクチャ","2. アーキテクチャ",{"depth":29,"slug":139,"text":140},"3-ファインチューニング","3. ファインチューニング",{"depth":24,"slug":87,"text":87},{"depth":29,"slug":143,"text":144},"glue-benchmark","GLUE Benchmark",{"depth":29,"slug":146,"text":147},"squad-11","SQuAD 1.1",{"depth":29,"slug":149,"text":150},"squad-20","SQuAD 2.0",{"depth":24,"slug":152,"text":152},"アブレーション研究",{"depth":29,"slug":154,"text":155},"nspの効果","NSPの効果",{"depth":29,"slug":157,"text":157},"双方向性の重要性",{"depth":29,"slug":159,"text":159},"モデルサイズの影響",{"depth":24,"slug":93,"text":93},{"depth":29,"slug":162,"text":162},"後続モデル",{"depth":29,"slug":164,"text":164},"応用分野",{"depth":24,"slug":166,"text":166},"個人的な感想",{"depth":24,"slug":168,"text":168},"今後の課題",[],[],{"title":106,"date":107},[],"bert.md","resnet",{"id":174,"data":176,"body":179,"filePath":180,"digest":181,"rendered":182,"legacyId":242},{"title":177,"date":178},"ResNet: Deep Residual Learning for Image Recognition","2023-12-10","## はじめに\n\nResNet（Residual Network）は、2016年にMicrosoft Researchから発表された深層畳み込みニューラルネットワークのアーキテクチャです。この研究は、深層学習における最も重要なブレークスルーの一つとして認識されています。\n\n## 背景と動機\n\n### 深いネットワークの課題\n\n従来の深層ニューラルネットワークでは、以下の問題が指摘されていました：\n\n1. **勾配消失問題**: ネットワークが深くなるほど、逆伝播時に勾配が小さくなってしまう\n2. **最適化の困難**: 深いネットワークの学習は非常に困難\n3. **性能の飽和**: 一定の深さを超えると性能が向上しない、または悪化する\n\n### 重要な観察\n\n著者らは重要な観察を行いました：\n\n> 理論的には、より深いネットワークは浅いネットワークの性能を下回ることはないはずである（浅いネットワークの構造を含むため）\n\nしかし実際には、深いネットワークの方が性能が悪くなることが多く観察されていました。\n\n## 残差学習の提案\n\n### 残差ブロックの設計\n\nResNetの核心的アイデアは、**残差学習（Residual Learning）**です：\n\n```\nH(x) = F(x) + x\n```\n\nここで：\n- `H(x)`: 理想的なマッピング\n- `F(x)`: 残差関数（学習対象）\n- `x`: 入力\n\n### Skip Connection\n\n![ResNet Block Architecture]\n\n残差ブロックは以下の要素から構成されます：\n\n1. **メインパス**: 畳み込み層、バッチ正規化、活性化関数\n2. **ショートカット接続**: 入力を出力に直接加算\n3. **最終活性化**: 残差と入力の和に対する活性化関数\n\n```python\ndef residual_block(x):\n    # メインパス\n    F_x = conv_bn_relu(x)\n    F_x = conv_bn(F_x)\n    \n    # ショートカット接続\n    H_x = F_x + x\n    \n    # 最終活性化\n    return relu(H_x)\n```\n\n## アーキテクチャの詳細\n\n### ResNet-50の構造\n\n| 層 | 出力サイズ | 残差ブロック |\n|---|-----------|-------------|\n| conv1 | 112×112 | 7×7, 64, stride 2 |\n| conv2_x | 56×56 | 3×3 max pool, stride 2\u003Cbr>[1×1, 64; 3×3, 64; 1×1, 256] × 3 |\n| conv3_x | 28×28 | [1×1, 128; 3×3, 128; 1×1, 512] × 4 |\n| conv4_x | 14×14 | [1×1, 256; 3×3, 256; 1×1, 1024] × 6 |\n| conv5_x | 7×7 | [1×1, 512; 3×3, 512; 1×1, 2048] × 3 |\n| - | 1×1 | average pool, 1000-d fc, softmax |\n\n### ボトルネック設計\n\n深いネットワーク（ResNet-50/101/152）では、計算効率のためボトルネック設計を採用：\n\n- **1×1 conv**: チャンネル数を削減\n- **3×3 conv**: 特徴抽出\n- **1×1 conv**: チャンネル数を復元\n\n## 実験結果\n\n### ImageNet分類\n\n| Model | Top-1 Error | Top-5 Error | Parameters |\n|-------|-------------|-------------|------------|\n| VGG-16 | - | 7.3% | 138M |\n| ResNet-50 | 24.7% | 7.8% | 25.6M |\n| ResNet-101 | 23.6% | 7.1% | 44.5M |\n| ResNet-152 | 23.0% | **6.7%** | 60.2M |\n\n### 他のタスクでの性能\n\n1. **CIFAR-10**: エラー率6.43%\n2. **Pascal VOC 2007**: mAP 76.4%\n3. **MS COCO**: 様々な検出タスクで最高性能\n\n## 技術的詳細\n\n### 実装のポイント\n\n1. **初期化**: Heの初期化を使用\n2. **バッチ正規化**: 各畳み込み層の後に適用\n3. **学習率スケジューリング**: エポック数に応じて学習率を調整\n4. **データ拡張**: ランダムクロッピング、水平反転など\n\n### 学習戦略\n\n```python\n# 学習設定例\noptimizer = SGD(lr=0.1, momentum=0.9, weight_decay=1e-4)\nscheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n```\n\n## 理論的解析\n\n### 勾配の流れ\n\n残差接続により、勾配は以下のように流れます：\n\n```\n∂loss/∂x = ∂loss/∂H(x) × (1 + ∂F(x)/∂x)\n```\n\nこれにより、勾配消失が防がれます。\n\n### 最適化の観点\n\n残差学習は、恒等写像の学習を容易にします：\n- もし恒等写像が最適なら、`F(x) = 0`を学習すれば良い\n- これは通常の学習よりも簡単\n\n## 影響と応用\n\n### 後続研究への影響\n\nResNetの登場により、以下の研究が活発化：\n\n1. **DenseNet**: より密な接続構造\n2. **Highway Networks**: ゲート機構付きの skip connection\n3. **ResNeXt**: 複数パスの残差学習\n4. **SE-Net**: チャンネル注意機構の導入\n\n### 実用的応用\n\n- **コンピュータビジョン**: 画像分類、物体検出、セグメンテーション\n- **医療画像解析**: 診断支援システム\n- **自動運転**: 画像認識システム\n- **産業応用**: 品質検査、異常検知\n\n## 実装例\n\n### PyTorchでの基本的な実装\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, \n                              kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, \n                              kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, \n                         kernel_size=1, stride=stride),\n                nn.BatchNorm2d(out_channels)\n            )\n    \n    def forward(self, x):\n        residual = self.shortcut(x)\n        out = torch.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        out = torch.relu(out)\n        return out\n```\n\n## 個人的考察\n\nResNetは、深層学習の発展において真のパラダイムシフトをもたらしました。単純ながら効果的なアイデアにより、それまで不可能だった極めて深いネットワークの学習を実現し、コンピュータビジョンタスクの性能を大幅に向上させました。\n\n特に印象的なのは、**シンプルさと効果の両立**です。skip connectionという直観的なアイデアが、勾配消失という根本的な問題を解決し、深層学習の実用性を飛躍的に高めました。\n\nこの研究により、現在に至るまで深層ニューラルネットワークの標準的な構成要素として残差接続が広く使われており、AIの実用化に大きく貢献しています。\n\n## 参考文献\n\n1. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. CVPR.\n2. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks. ECCV.\n3. Szegedy, C., et al. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. AAAI.","src/content/papers/resnet.md","225f6337984d9946",{"html":183,"metadata":184},"\u003Ch2 id=\"はじめに\">はじめに\u003C/h2>\n\u003Cp>ResNet（Residual Network）は、2016年にMicrosoft Researchから発表された深層畳み込みニューラルネットワークのアーキテクチャです。この研究は、深層学習における最も重要なブレークスルーの一つとして認識されています。\u003C/p>\n\u003Ch2 id=\"背景と動機\">背景と動機\u003C/h2>\n\u003Ch3 id=\"深いネットワークの課題\">深いネットワークの課題\u003C/h3>\n\u003Cp>従来の深層ニューラルネットワークでは、以下の問題が指摘されていました：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>勾配消失問題\u003C/strong>: ネットワークが深くなるほど、逆伝播時に勾配が小さくなってしまう\u003C/li>\n\u003Cli>\u003Cstrong>最適化の困難\u003C/strong>: 深いネットワークの学習は非常に困難\u003C/li>\n\u003Cli>\u003Cstrong>性能の飽和\u003C/strong>: 一定の深さを超えると性能が向上しない、または悪化する\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"重要な観察\">重要な観察\u003C/h3>\n\u003Cp>著者らは重要な観察を行いました：\u003C/p>\n\u003Cblockquote>\n\u003Cp>理論的には、より深いネットワークは浅いネットワークの性能を下回ることはないはずである（浅いネットワークの構造を含むため）\u003C/p>\n\u003C/blockquote>\n\u003Cp>しかし実際には、深いネットワークの方が性能が悪くなることが多く観察されていました。\u003C/p>\n\u003Ch2 id=\"残差学習の提案\">残差学習の提案\u003C/h2>\n\u003Ch3 id=\"残差ブロックの設計\">残差ブロックの設計\u003C/h3>\n\u003Cp>ResNetの核心的アイデアは、**残差学習（Residual Learning）**です：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>H(x) = F(x) + x\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>ここで：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Ccode>H(x)\u003C/code>: 理想的なマッピング\u003C/li>\n\u003Cli>\u003Ccode>F(x)\u003C/code>: 残差関数（学習対象）\u003C/li>\n\u003Cli>\u003Ccode>x\u003C/code>: 入力\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"skip-connection\">Skip Connection\u003C/h3>\n\u003Cp>![ResNet Block Architecture]\u003C/p>\n\u003Cp>残差ブロックは以下の要素から構成されます：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>メインパス\u003C/strong>: 畳み込み層、バッチ正規化、活性化関数\u003C/li>\n\u003Cli>\u003Cstrong>ショートカット接続\u003C/strong>: 入力を出力に直接加算\u003C/li>\n\u003Cli>\u003Cstrong>最終活性化\u003C/strong>: 残差と入力の和に対する活性化関数\u003C/li>\n\u003C/ol>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">def\u003C/span>\u003Cspan style=\"color:#B392F0\"> residual_block\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(x):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # メインパス\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    F_x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> conv_bn_relu(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    F_x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> conv_bn(F_x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # ショートカット接続\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    H_x \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> F_x \u003C/span>\u003Cspan style=\"color:#F97583\">+\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> x\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\">    # 最終活性化\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> relu(H_x)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"アーキテクチャの詳細\">アーキテクチャの詳細\u003C/h2>\n\u003Ch3 id=\"resnet-50の構造\">ResNet-50の構造\u003C/h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>層\u003C/th>\u003Cth>出力サイズ\u003C/th>\u003Cth>残差ブロック\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>conv1\u003C/td>\u003Ctd>112×112\u003C/td>\u003Ctd>7×7, 64, stride 2\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>conv2_x\u003C/td>\u003Ctd>56×56\u003C/td>\u003Ctd>3×3 max pool, stride 2\u003Cbr>[1×1, 64; 3×3, 64; 1×1, 256] × 3\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>conv3_x\u003C/td>\u003Ctd>28×28\u003C/td>\u003Ctd>[1×1, 128; 3×3, 128; 1×1, 512] × 4\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>conv4_x\u003C/td>\u003Ctd>14×14\u003C/td>\u003Ctd>[1×1, 256; 3×3, 256; 1×1, 1024] × 6\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>conv5_x\u003C/td>\u003Ctd>7×7\u003C/td>\u003Ctd>[1×1, 512; 3×3, 512; 1×1, 2048] × 3\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>-\u003C/td>\u003Ctd>1×1\u003C/td>\u003Ctd>average pool, 1000-d fc, softmax\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch3 id=\"ボトルネック設計\">ボトルネック設計\u003C/h3>\n\u003Cp>深いネットワーク（ResNet-50/101/152）では、計算効率のためボトルネック設計を採用：\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>1×1 conv\u003C/strong>: チャンネル数を削減\u003C/li>\n\u003Cli>\u003Cstrong>3×3 conv\u003C/strong>: 特徴抽出\u003C/li>\n\u003Cli>\u003Cstrong>1×1 conv\u003C/strong>: チャンネル数を復元\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"実験結果\">実験結果\u003C/h2>\n\u003Ch3 id=\"imagenet分類\">ImageNet分類\u003C/h3>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Model\u003C/th>\u003Cth>Top-1 Error\u003C/th>\u003Cth>Top-5 Error\u003C/th>\u003Cth>Parameters\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>VGG-16\u003C/td>\u003Ctd>-\u003C/td>\u003Ctd>7.3%\u003C/td>\u003Ctd>138M\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>ResNet-50\u003C/td>\u003Ctd>24.7%\u003C/td>\u003Ctd>7.8%\u003C/td>\u003Ctd>25.6M\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>ResNet-101\u003C/td>\u003Ctd>23.6%\u003C/td>\u003Ctd>7.1%\u003C/td>\u003Ctd>44.5M\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>ResNet-152\u003C/td>\u003Ctd>23.0%\u003C/td>\u003Ctd>\u003Cstrong>6.7%\u003C/strong>\u003C/td>\u003Ctd>60.2M\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Ch3 id=\"他のタスクでの性能\">他のタスクでの性能\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>CIFAR-10\u003C/strong>: エラー率6.43%\u003C/li>\n\u003Cli>\u003Cstrong>Pascal VOC 2007\u003C/strong>: mAP 76.4%\u003C/li>\n\u003Cli>\u003Cstrong>MS COCO\u003C/strong>: 様々な検出タスクで最高性能\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"技術的詳細\">技術的詳細\u003C/h2>\n\u003Ch3 id=\"実装のポイント\">実装のポイント\u003C/h3>\n\u003Col>\n\u003Cli>\u003Cstrong>初期化\u003C/strong>: Heの初期化を使用\u003C/li>\n\u003Cli>\u003Cstrong>バッチ正規化\u003C/strong>: 各畳み込み層の後に適用\u003C/li>\n\u003Cli>\u003Cstrong>学習率スケジューリング\u003C/strong>: エポック数に応じて学習率を調整\u003C/li>\n\u003Cli>\u003Cstrong>データ拡張\u003C/strong>: ランダムクロッピング、水平反転など\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"学習戦略\">学習戦略\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#6A737D\"># 学習設定例\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">optimizer \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> SGD(\u003C/span>\u003Cspan style=\"color:#FFAB70\">lr\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">momentum\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.9\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">weight_decay\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1e-4\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">scheduler \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> MultiStepLR(optimizer, \u003C/span>\u003Cspan style=\"color:#FFAB70\">milestones\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">[\u003C/span>\u003Cspan style=\"color:#79B8FF\">30\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">60\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#79B8FF\">90\u003C/span>\u003Cspan style=\"color:#E1E4E8\">], \u003C/span>\u003Cspan style=\"color:#FFAB70\">gamma\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">0.1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"理論的解析\">理論的解析\u003C/h2>\n\u003Ch3 id=\"勾配の流れ\">勾配の流れ\u003C/h3>\n\u003Cp>残差接続により、勾配は以下のように流れます：\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan>∂loss/∂x = ∂loss/∂H(x) × (1 + ∂F(x)/∂x)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>これにより、勾配消失が防がれます。\u003C/p>\n\u003Ch3 id=\"最適化の観点\">最適化の観点\u003C/h3>\n\u003Cp>残差学習は、恒等写像の学習を容易にします：\u003C/p>\n\u003Cul>\n\u003Cli>もし恒等写像が最適なら、\u003Ccode>F(x) = 0\u003C/code>を学習すれば良い\u003C/li>\n\u003Cli>これは通常の学習よりも簡単\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"影響と応用\">影響と応用\u003C/h2>\n\u003Ch3 id=\"後続研究への影響\">後続研究への影響\u003C/h3>\n\u003Cp>ResNetの登場により、以下の研究が活発化：\u003C/p>\n\u003Col>\n\u003Cli>\u003Cstrong>DenseNet\u003C/strong>: より密な接続構造\u003C/li>\n\u003Cli>\u003Cstrong>Highway Networks\u003C/strong>: ゲート機構付きの skip connection\u003C/li>\n\u003Cli>\u003Cstrong>ResNeXt\u003C/strong>: 複数パスの残差学習\u003C/li>\n\u003Cli>\u003Cstrong>SE-Net\u003C/strong>: チャンネル注意機構の導入\u003C/li>\n\u003C/ol>\n\u003Ch3 id=\"実用的応用\">実用的応用\u003C/h3>\n\u003Cul>\n\u003Cli>\u003Cstrong>コンピュータビジョン\u003C/strong>: 画像分類、物体検出、セグメンテーション\u003C/li>\n\u003Cli>\u003Cstrong>医療画像解析\u003C/strong>: 診断支援システム\u003C/li>\n\u003Cli>\u003Cstrong>自動運転\u003C/strong>: 画像認識システム\u003C/li>\n\u003Cli>\u003Cstrong>産業応用\u003C/strong>: 品質検査、異常検知\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"実装例\">実装例\u003C/h2>\n\u003Ch3 id=\"pytorchでの基本的な実装\">PyTorchでの基本的な実装\u003C/h3>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">import\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.nn \u003C/span>\u003Cspan style=\"color:#F97583\">as\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">class\u003C/span>\u003Cspan style=\"color:#B392F0\"> BasicBlock\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#B392F0\">nn\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.\u003C/span>\u003Cspan style=\"color:#B392F0\">Module\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#79B8FF\"> __init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, in_channels, out_channels, stride\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        super\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(BasicBlock, \u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">).\u003C/span>\u003Cspan style=\"color:#79B8FF\">__init__\u003C/span>\u003Cspan style=\"color:#E1E4E8\">()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Conv2d(in_channels, out_channels, \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                              kernel_size\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">stride\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">stride, \u003C/span>\u003Cspan style=\"color:#FFAB70\">padding\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn1 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.BatchNorm2d(out_channels)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Conv2d(out_channels, out_channels, \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                              kernel_size\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">3\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">stride\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">padding\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn2 \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.BatchNorm2d(out_channels)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">        self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.shortcut \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Sequential()\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        if\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> stride \u003C/span>\u003Cspan style=\"color:#F97583\">!=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> 1\u003C/span>\u003Cspan style=\"color:#F97583\"> or\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> in_channels \u003C/span>\u003Cspan style=\"color:#F97583\">!=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> out_channels:\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#79B8FF\">            self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.shortcut \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> nn.Sequential(\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                nn.Conv2d(in_channels, out_channels, \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#FFAB70\">                         kernel_size\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\">1\u003C/span>\u003Cspan style=\"color:#E1E4E8\">, \u003C/span>\u003Cspan style=\"color:#FFAB70\">stride\u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">stride),\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">                nn.BatchNorm2d(out_channels)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">            )\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">    \u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">    def\u003C/span>\u003Cspan style=\"color:#B392F0\"> forward\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(self, x):\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        residual \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.shortcut(x)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        out \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.relu(\u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn1(\u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv1(x)))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        out \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#79B8FF\"> self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.bn2(\u003C/span>\u003Cspan style=\"color:#79B8FF\">self\u003C/span>\u003Cspan style=\"color:#E1E4E8\">.conv2(out))\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        out \u003C/span>\u003Cspan style=\"color:#F97583\">+=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> residual\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">        out \u003C/span>\u003Cspan style=\"color:#F97583\">=\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> torch.relu(out)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#F97583\">        return\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> out\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"個人的考察\">個人的考察\u003C/h2>\n\u003Cp>ResNetは、深層学習の発展において真のパラダイムシフトをもたらしました。単純ながら効果的なアイデアにより、それまで不可能だった極めて深いネットワークの学習を実現し、コンピュータビジョンタスクの性能を大幅に向上させました。\u003C/p>\n\u003Cp>特に印象的なのは、\u003Cstrong>シンプルさと効果の両立\u003C/strong>です。skip connectionという直観的なアイデアが、勾配消失という根本的な問題を解決し、深層学習の実用性を飛躍的に高めました。\u003C/p>\n\u003Cp>この研究により、現在に至るまで深層ニューラルネットワークの標準的な構成要素として残差接続が広く使われており、AIの実用化に大きく貢献しています。\u003C/p>\n\u003Ch2 id=\"参考文献\">参考文献\u003C/h2>\n\u003Col>\n\u003Cli>He, K., Zhang, X., Ren, S., &#x26; Sun, J. (2016). Deep residual learning for image recognition. CVPR.\u003C/li>\n\u003Cli>He, K., Zhang, X., Ren, S., &#x26; Sun, J. (2016). Identity mappings in deep residual networks. ECCV.\u003C/li>\n\u003Cli>Szegedy, C., et al. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. AAAI.\u003C/li>\n\u003C/ol>",{"headings":185,"localImagePaths":238,"remoteImagePaths":239,"frontmatter":240,"imagePaths":241},[186,188,189,191,193,195,197,200,202,205,207,208,211,213,215,217,219,221,223,225,226,228,229,231,234,236],{"depth":24,"slug":187,"text":187},"はじめに",{"depth":24,"slug":118,"text":118},{"depth":29,"slug":190,"text":190},"深いネットワークの課題",{"depth":29,"slug":192,"text":192},"重要な観察",{"depth":24,"slug":194,"text":194},"残差学習の提案",{"depth":29,"slug":196,"text":196},"残差ブロックの設計",{"depth":29,"slug":198,"text":199},"skip-connection","Skip Connection",{"depth":24,"slug":201,"text":201},"アーキテクチャの詳細",{"depth":29,"slug":203,"text":204},"resnet-50の構造","ResNet-50の構造",{"depth":29,"slug":206,"text":206},"ボトルネック設計",{"depth":24,"slug":87,"text":87},{"depth":29,"slug":209,"text":210},"imagenet分類","ImageNet分類",{"depth":29,"slug":212,"text":212},"他のタスクでの性能",{"depth":24,"slug":214,"text":214},"技術的詳細",{"depth":29,"slug":216,"text":216},"実装のポイント",{"depth":29,"slug":218,"text":218},"学習戦略",{"depth":24,"slug":220,"text":220},"理論的解析",{"depth":29,"slug":222,"text":222},"勾配の流れ",{"depth":29,"slug":224,"text":224},"最適化の観点",{"depth":24,"slug":93,"text":93},{"depth":29,"slug":227,"text":227},"後続研究への影響",{"depth":29,"slug":40,"text":40},{"depth":24,"slug":230,"text":230},"実装例",{"depth":29,"slug":232,"text":233},"pytorchでの基本的な実装","PyTorchでの基本的な実装",{"depth":24,"slug":235,"text":235},"個人的考察",{"depth":24,"slug":237,"text":237},"参考文献",[],[],{"title":177,"date":178},[],"resnet.md"]