[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.0","content-config-digest","6ab83d4c224a4955","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[null],\"rehypePlugins\":[null],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","papers",["Map",11,12,64,65,107,108],"sudden-drops-in-the-loss-_-syntax-acquisition-phase-transitions-and-simplicity-bias-in-mlms-2024",{"id":11,"data":13,"body":16,"filePath":17,"digest":18,"rendered":19,"legacyId":63},{"title":14,"date":15},"Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs (2024)","2025-06-08","## arXiv\nhttps://arxiv.org/abs/2309.07311\n\n## 著者\n[Angelica Chen](https://openreview.net/profile?id=~Angelica_Chen1), [Ravid Shwartz-Ziv](https://openreview.net/profile?id=~Ravid_Shwartz-Ziv2), [Kyunghyun Cho](https://openreview.net/profile?id=~Kyunghyun_Cho1), [Matthew L Leavitt](https://openreview.net/profile?id=~Matthew_L_Leavitt1), [Naomi Saphra](https://openreview.net/profile?id=~Naomi_Saphra1)\n\n## 要約\n- BERTとかのMLMの学習過程で、短い期間でSASを獲得する傾向がある\n- SASを獲得すると損失が急激に低下し、言語能力の獲得を促進\n- SASは訓練中に操作可能\n\t- 抑制すると複雑な言語能力の出現が妨げられる\n\t- 初期段階で一時的に抑制するとモデルの品質向上、収束の加速が見られた\n\n## 用語とか\n### MLM (Masked Language Model)\n- 文章中の一部の単語を隠して、その隠された単語を予測させるタスクを通じて言語を学習するモデル\n- BERTなど\n- 文の前後両方の文脈を同時に考慮できる\n### SAS(Syntactic Attention Structure)\n- モデルが特定の構文的な依存関係に注目したアテンションヘッドを形成する傾向\n- MLMの学習時に明示的な帰納バイアスなしに自然発生\n- SASの発現を制御→MLMの内部構造の特性と外的な能力の関係を観察\n![SASの概念図](/images/スクリーンショット%202025-06-08%2021.04.25.png)\n\n### UAS(unlabeld attatchment score)\n- SASの定量化\n- 言語モデルが構文解析の結果と同じように単語にアテンションを当てられているか\n- 構文解析の結果と比較して予測が成功した割合を計算\n### 相転移\n- 非連続的な過程\n- 知識の発見はスケーリング則に従わず唐突な変化を見せる\n\n## SASを調整する手法\n$$\nL(x) = L_{MLM}(x) + \\lambda \\sum^{|x|}_{i=1} \\sum_{x_{j} \\in D(x_i)} \\gamma(x_i, x_j)\n$$\n- 構文性スコア$\\gamma(x_i, x_j)$を用いた正則化項を$\\lambda$でスケーリング\n\t- $\\lambda \u003C 0$: SASを促進\n\t- $\\lambda > 0$: SASを抑制\n- $\\gamma(x_i, x_j)$: 構文的に関係のある単語$i,j$間の最大アテンションの重み\n\n## SASを初期段階で抑制するとモデル品質が向上する理由\n- 代替戦略の獲得\n\t- ざっくりSASじゃない戦略のこと\n\t- 長距離の文脈を利用する戦略\n- SASをよりよく学ぶための踏み台的な\n\n## 結論\n### 単純性バイアスとの関係\n- モデルは学習初期でSASのような解釈しやすく単純な回を好む傾向があり\n- これに固執すると長期的な性能向上を妨げる可能性\n### 学習の臨界点\n- フェーズ遷移(損失が急落するところ)が起きている最中に学習方法を変更するとモデルの性能が最も下がる\n- 臨界点は非常に不安定\n\n## 感想\n- 学習過程での構文の獲得過程みたいなものが見れておもろい\n- 最初はSASを抑制した方がいいみたいなのはwarmupとかと繋がるのかな？\n- 長期的に性能をあげるための工夫が大事","src/content/papers/Sudden Drops in the Loss _ Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs (2024).md","45a5f61c8070786e",{"html":20,"metadata":21},"\u003Ch2 id=\"arxiv\">arXiv\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/abs/2309.07311\">https://arxiv.org/abs/2309.07311\u003C/a>\u003C/p>\n\u003Ch2 id=\"著者\">著者\u003C/h2>\n\u003Cp>\u003Ca href=\"https://openreview.net/profile?id=~Angelica_Chen1\">Angelica Chen\u003C/a>, \u003Ca href=\"https://openreview.net/profile?id=~Ravid_Shwartz-Ziv2\">Ravid Shwartz-Ziv\u003C/a>, \u003Ca href=\"https://openreview.net/profile?id=~Kyunghyun_Cho1\">Kyunghyun Cho\u003C/a>, \u003Ca href=\"https://openreview.net/profile?id=~Matthew_L_Leavitt1\">Matthew L Leavitt\u003C/a>, \u003Ca href=\"https://openreview.net/profile?id=~Naomi_Saphra1\">Naomi Saphra\u003C/a>\u003C/p>\n\u003Ch2 id=\"要約\">要約\u003C/h2>\n\u003Cul>\n\u003Cli>BERTとかのMLMの学習過程で、短い期間でSASを獲得する傾向がある\u003C/li>\n\u003Cli>SASを獲得すると損失が急激に低下し、言語能力の獲得を促進\u003C/li>\n\u003Cli>SASは訓練中に操作可能\n\u003Cul>\n\u003Cli>抑制すると複雑な言語能力の出現が妨げられる\u003C/li>\n\u003Cli>初期段階で一時的に抑制するとモデルの品質向上、収束の加速が見られた\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"用語とか\">用語とか\u003C/h2>\n\u003Ch3 id=\"mlm-masked-language-model\">MLM (Masked Language Model)\u003C/h3>\n\u003Cul>\n\u003Cli>文章中の一部の単語を隠して、その隠された単語を予測させるタスクを通じて言語を学習するモデル\u003C/li>\n\u003Cli>BERTなど\u003C/li>\n\u003Cli>文の前後両方の文脈を同時に考慮できる\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"sassyntactic-attention-structure\">SAS(Syntactic Attention Structure)\u003C/h3>\n\u003Cul>\n\u003Cli>モデルが特定の構文的な依存関係に注目したアテンションヘッドを形成する傾向\u003C/li>\n\u003Cli>MLMの学習時に明示的な帰納バイアスなしに自然発生\u003C/li>\n\u003Cli>SASの発現を制御→MLMの内部構造の特性と外的な能力の関係を観察\n\u003Cimg src=\"/images/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202025-06-08%2021.04.25.png\" alt=\"SASの概念図\">\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"uasunlabeld-attatchment-score\">UAS(unlabeld attatchment score)\u003C/h3>\n\u003Cul>\n\u003Cli>SASの定量化\u003C/li>\n\u003Cli>言語モデルが構文解析の結果と同じように単語にアテンションを当てられているか\u003C/li>\n\u003Cli>構文解析の結果と比較して予測が成功した割合を計算\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"相転移\">相転移\u003C/h3>\n\u003Cul>\n\u003Cli>非連続的な過程\u003C/li>\n\u003Cli>知識の発見はスケーリング則に従わず唐突な変化を見せる\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"sasを調整する手法\">SASを調整する手法\u003C/h2>\n\u003Cspan class=\"katex-display\">\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\u003Csemantics>\u003Cmrow>\u003Cmi>L\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>x\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo>=\u003C/mo>\u003Cmsub>\u003Cmi>L\u003C/mi>\u003Cmrow>\u003Cmi>M\u003C/mi>\u003Cmi>L\u003C/mi>\u003Cmi>M\u003C/mi>\u003C/mrow>\u003C/msub>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmi>x\u003C/mi>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003Cmo>+\u003C/mo>\u003Cmi>λ\u003C/mi>\u003Cmunderover>\u003Cmo>∑\u003C/mo>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003Cmo>=\u003C/mo>\u003Cmn>1\u003C/mn>\u003C/mrow>\u003Cmrow>\u003Cmi mathvariant=\"normal\">∣\u003C/mi>\u003Cmi>x\u003C/mi>\u003Cmi mathvariant=\"normal\">∣\u003C/mi>\u003C/mrow>\u003C/munderover>\u003Cmunder>\u003Cmo>∑\u003C/mo>\u003Cmrow>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>j\u003C/mi>\u003C/msub>\u003Cmo>∈\u003C/mo>\u003Cmi>D\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003C/munder>\u003Cmi>γ\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>j\u003C/mi>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">L(x) = L_{MLM}(x) + \\lambda \\sum^{|x|}_{i=1} \\sum_{x_{j} \\in D(x_i)} \\gamma(x_i, x_j)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">L\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">=\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">L\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3283em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">M\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">L\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.10903em;\">M\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003Cspan class=\"mbin\">+\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2222em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:3.4993em;vertical-align:-1.5383em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">λ\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop op-limits\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.961em;\">\u003Cspan style=\"top:-1.8723em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003Cspan class=\"mrel mtight\">=\u003C/span>\u003Cspan class=\"mord mtight\">1\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.05em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan>\u003Cspan class=\"mop op-symbol large-op\">∑\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-4.386em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">∣\u003C/span>\u003Cspan class=\"mord mathnormal mtight\">x\u003C/span>\u003Cspan class=\"mord mtight\">∣\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.2777em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mop op-limits\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.05em;\">\u003Cspan style=\"top:-1.809em;margin-left:0em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3281em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2819em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mrel mtight\">∈\u003C/span>\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D\u003C/span>\u003Cspan class=\"mopen mtight\">(\u003C/span>\u003Cspan class=\"mord mtight\">\u003Cspan class=\"mord mathnormal mtight\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3281em;\">\u003Cspan style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\">\u003Cspan class=\"pstrut\" style=\"height:2.5em;\">\u003C/span>\u003Cspan class=\"sizing reset-size3 size1 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.143em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose mtight\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan style=\"top:-3.05em;\">\u003Cspan class=\"pstrut\" style=\"height:3.05em;\">\u003C/span>\u003Cspan>\u003Cspan class=\"mop op-symbol large-op\">∑\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:1.5383em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2861em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\n\u003Cul>\n\u003Cli>構文性スコア\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>γ\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>j\u003C/mi>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\gamma(x_i, x_j)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2861em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>を用いた正則化項を\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>λ\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\lambda\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">λ\u003C/span>\u003C/span>\u003C/span>\u003C/span>でスケーリング\n\u003Cul>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>λ\u003C/mi>\u003Cmo>&#x3C;\u003C/mo>\u003Cmn>0\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\lambda &#x3C; 0\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7335em;vertical-align:-0.0391em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">λ\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">&#x3C;\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>: SASを促進\u003C/li>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>λ\u003C/mi>\u003Cmo>>\u003C/mo>\u003Cmn>0\u003C/mn>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\lambda > 0\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.7335em;vertical-align:-0.0391em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">λ\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003Cspan class=\"mrel\">>\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.2778em;\">\u003C/span>\u003C/span>\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.6444em;\">\u003C/span>\u003Cspan class=\"mord\">0\u003C/span>\u003C/span>\u003C/span>\u003C/span>: SASを抑制\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>γ\u003C/mi>\u003Cmo stretchy=\"false\">(\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>i\u003C/mi>\u003C/msub>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmsub>\u003Cmi>x\u003C/mi>\u003Cmi>j\u003C/mi>\u003C/msub>\u003Cmo stretchy=\"false\">)\u003C/mo>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">\\gamma(x_i, x_j)\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:1.0361em;vertical-align:-0.2861em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ\u003C/span>\u003Cspan class=\"mopen\">(\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\">i\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.15em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord\">\u003Cspan class=\"mord mathnormal\">x\u003C/span>\u003Cspan class=\"msupsub\">\u003Cspan class=\"vlist-t vlist-t2\">\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.3117em;\">\u003Cspan style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\">\u003Cspan class=\"pstrut\" style=\"height:2.7em;\">\u003C/span>\u003Cspan class=\"sizing reset-size6 size3 mtight\">\u003Cspan class=\"mord mathnormal mtight\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"vlist-s\">​\u003C/span>\u003C/span>\u003Cspan class=\"vlist-r\">\u003Cspan class=\"vlist\" style=\"height:0.2861em;\">\u003Cspan>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003C/span>\u003Cspan class=\"mclose\">)\u003C/span>\u003C/span>\u003C/span>\u003C/span>: 構文的に関係のある単語\u003Cspan class=\"katex\">\u003Cspan class=\"katex-mathml\">\u003Cmath xmlns=\"http://www.w3.org/1998/Math/MathML\">\u003Csemantics>\u003Cmrow>\u003Cmi>i\u003C/mi>\u003Cmo separator=\"true\">,\u003C/mo>\u003Cmi>j\u003C/mi>\u003C/mrow>\u003Cannotation encoding=\"application/x-tex\">i,j\u003C/annotation>\u003C/semantics>\u003C/math>\u003C/span>\u003Cspan class=\"katex-html\" aria-hidden=\"true\">\u003Cspan class=\"base\">\u003Cspan class=\"strut\" style=\"height:0.854em;vertical-align:-0.1944em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\">i\u003C/span>\u003Cspan class=\"mpunct\">,\u003C/span>\u003Cspan class=\"mspace\" style=\"margin-right:0.1667em;\">\u003C/span>\u003Cspan class=\"mord mathnormal\" style=\"margin-right:0.05724em;\">j\u003C/span>\u003C/span>\u003C/span>\u003C/span>間の最大アテンションの重み\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"sasを初期段階で抑制するとモデル品質が向上する理由\">SASを初期段階で抑制するとモデル品質が向上する理由\u003C/h2>\n\u003Cul>\n\u003Cli>代替戦略の獲得\n\u003Cul>\n\u003Cli>ざっくりSASじゃない戦略のこと\u003C/li>\n\u003Cli>長距離の文脈を利用する戦略\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>SASをよりよく学ぶための踏み台的な\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"結論\">結論\u003C/h2>\n\u003Ch3 id=\"単純性バイアスとの関係\">単純性バイアスとの関係\u003C/h3>\n\u003Cul>\n\u003Cli>モデルは学習初期でSASのような解釈しやすく単純な回を好む傾向があり\u003C/li>\n\u003Cli>これに固執すると長期的な性能向上を妨げる可能性\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"学習の臨界点\">学習の臨界点\u003C/h3>\n\u003Cul>\n\u003Cli>フェーズ遷移(損失が急落するところ)が起きている最中に学習方法を変更するとモデルの性能が最も下がる\u003C/li>\n\u003Cli>臨界点は非常に不安定\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"感想\">感想\u003C/h2>\n\u003Cul>\n\u003Cli>学習過程での構文の獲得過程みたいなものが見れておもろい\u003C/li>\n\u003Cli>最初はSASを抑制した方がいいみたいなのはwarmupとかと繋がるのかな？\u003C/li>\n\u003Cli>長期的に性能をあげるための工夫が大事\u003C/li>\n\u003C/ul>",{"headings":22,"localImagePaths":59,"remoteImagePaths":60,"frontmatter":61,"imagePaths":62},[23,27,29,31,33,37,40,43,45,48,51,53,55,57],{"depth":24,"slug":25,"text":26},2,"arxiv","arXiv",{"depth":24,"slug":28,"text":28},"著者",{"depth":24,"slug":30,"text":30},"要約",{"depth":24,"slug":32,"text":32},"用語とか",{"depth":34,"slug":35,"text":36},3,"mlm-masked-language-model","MLM (Masked Language Model)",{"depth":34,"slug":38,"text":39},"sassyntactic-attention-structure","SAS(Syntactic Attention Structure)",{"depth":34,"slug":41,"text":42},"uasunlabeld-attatchment-score","UAS(unlabeld attatchment score)",{"depth":34,"slug":44,"text":44},"相転移",{"depth":24,"slug":46,"text":47},"sasを調整する手法","SASを調整する手法",{"depth":24,"slug":49,"text":50},"sasを初期段階で抑制するとモデル品質が向上する理由","SASを初期段階で抑制するとモデル品質が向上する理由",{"depth":24,"slug":52,"text":52},"結論",{"depth":34,"slug":54,"text":54},"単純性バイアスとの関係",{"depth":34,"slug":56,"text":56},"学習の臨界点",{"depth":24,"slug":58,"text":58},"感想",[],[],{"title":14,"date":15},[],"Sudden Drops in the Loss _ Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs (2024).md","the-llama-3-herd-of-models2024",{"id":64,"data":66,"body":69,"filePath":70,"digest":71,"rendered":72,"legacyId":106},{"title":67,"date":68},"The Llama 3 Herd of Models (2024)","2025-06-07","## arXiv\nhttps://arxiv.org/abs/2407.21783\n\n## 要約\n- Llama-3に関する研究開発\n- 15.6Tトークンで学習された8B,70B,405Bのdense Transformer\n- 128Kトークンのコンテクストウィンドウ\n- **DPO！！！**\n- 4oとかClaude3.5とかに匹敵するレベル\n- Llama-2 70BとLlama-3 8Bが同じぐらいの性能\n\n## 学習方法\n1. 教師なし学習により、大量のテキストコーパスから学習\n2. 現在のモデルでプリファレンスデータ（出力を人手で比較評価したやつ）を作成し、報酬モデリングを実施\n3. 現在のモデル＋報酬モデルSFT用のデータを作成し、ベースモデルをチューニング\n4. プリファレンスデータをLLMにマッチさせるようにDPOでベストモデルを更新\n\n## 開発における3つの主要要素\n### データ\n- 事前、事後両方の学習でのデータの質と量が向上\n- データミックスは、一般知識50％、数学的推論25％、コード17％、多言語トークン8％\n- 安全性のためのデータフィルタリング\n### スケール\n- Llama 2の最大verより約50倍の計算量でモデルを訓練\n### 複雑性の管理\n- 標準のdense Transformerのアーキテクチャを採用\n- MoEは複雑だから避けて安定性重視\n\t- MoEってなんですかの人はこちら\n\t- https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\n\n## Llama 2からの変更点\n### PPO → DPO\n- ここで出てきますねDPO\n### データ量\n- 1.8兆tokens → 15兆tokens\n### モデルアーキテクチャの微変更\n- Grouped Query Attention(GQA)の採用\n\t- GQAのGemini解説: https://g.co/gemini/share/5a3c1c47f73d\n\t- 推論速度向上\n\t- デコード中のキャッシュサイズ削減\n- トークンボキャブラリの拡張\n\t- tiktokenトークナイザの100K+非英語言語用の28Kトークン\n\t- 英語データの圧縮率が3.17文字/token から3.94文字/token に向上\n\t\t- 同じ計算量でいっぱい読める\n\t- RoPEベースの周波数ハイパラが増加\n\t\t- RoPEは位置情報を位置の加算じゃなくて回転するやつ\n\t\t- 500,000に増加(何が嬉しいのかよくわからなかった)\n\n## 感想\n- もうDPOしか頭に入ってこなかった\n- RoPEのところの話があんまり理解できなかった\n- やっぱりLlamaは安全性にすごい気を使っていそう\n\n## 著者(多すぎ)\n[Aaron Grattafiori](https://arxiv.org/search/cs?searchtype=author&query=Grattafiori,+A), [Abhimanyu Dubey](https://arxiv.org/search/cs?searchtype=author&query=Dubey,+A), [Abhinav Jauhri](https://arxiv.org/search/cs?searchtype=author&query=Jauhri,+A), [Abhinav Pandey](https://arxiv.org/search/cs?searchtype=author&query=Pandey,+A), [Abhishek Kadian](https://arxiv.org/search/cs?searchtype=author&query=Kadian,+A), [Ahmad Al-Dahle](https://arxiv.org/search/cs?searchtype=author&query=Al-Dahle,+A), [Aiesha Letman](https://arxiv.org/search/cs?searchtype=author&query=Letman,+A), [Akhil Mathur](https://arxiv.org/search/cs?searchtype=author&query=Mathur,+A), [Alan Schelten](https://arxiv.org/search/cs?searchtype=author&query=Schelten,+A), [Alex Vaughan](https://arxiv.org/search/cs?searchtype=author&query=Vaughan,+A), [Amy Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+A), [Angela Fan](https://arxiv.org/search/cs?searchtype=author&query=Fan,+A), [Anirudh Goyal](https://arxiv.org/search/cs?searchtype=author&query=Goyal,+A), [Anthony Hartshorn](https://arxiv.org/search/cs?searchtype=author&query=Hartshorn,+A), [Aobo Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang,+A), [Archi Mitra](https://arxiv.org/search/cs?searchtype=author&query=Mitra,+A), [Archie Sravankumar](https://arxiv.org/search/cs?searchtype=author&query=Archie), [Artem Korenev](https://arxiv.org/search/cs?searchtype=author&query=Korenev,+A), [Arthur Hinsvark](https://arxiv.org/search/cs?searchtype=author&query=Hinsvark,+A), [Arun Rao](https://arxiv.org/search/cs?searchtype=author&query=Rao,+A), [Aston Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+A), [Aurelien Rodriguez](https://arxiv.org/search/cs?searchtype=author&query=Rodriguez,+A), [Austen Gregerson](https://arxiv.org/search/cs?searchtype=author&query=Gregerson,+A), [Ava Spataru](https://arxiv.org/search/cs?searchtype=author&query=Spataru,+A), [Baptiste Roziere](https://arxiv.org/search/cs?searchtype=author&query=Roziere,+B), [Bethany Biron](https://arxiv.org/search/cs?searchtype=author&query=Biron,+B), [Binh Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+B), [Bobbie Chern](https://arxiv.org/search/cs?searchtype=author&query=Chern,+B), [Charlotte Caucheteux](https://arxiv.org/search/cs?searchtype=author&query=Caucheteux,+C), [Chaya Nayak](https://arxiv.org/search/cs?searchtype=author&query=Nayak,+C), [Chloe Bi](https://arxiv.org/search/cs?searchtype=author&query=Bi,+C), [Chris Marra](https://arxiv.org/search/cs?searchtype=author&query=Marra,+C), [Chris McConnell](https://arxiv.org/search/cs?searchtype=author&query=McConnell,+C), [Christian Keller](https://arxiv.org/search/cs?searchtype=author&query=Keller,+C), [Christophe Touret](https://arxiv.org/search/cs?searchtype=author&query=Touret,+C), [Chunyang Wu](https://arxiv.org/search/cs?searchtype=author&query=Wu,+C), [Corinne Wong](https://arxiv.org/search/cs?searchtype=author&query=Wong,+C), [Cristian Canton Ferrer](https://arxiv.org/search/cs?searchtype=author&query=Ferrer,+C+C), [Cyrus Nikolaidis](https://arxiv.org/search/cs?searchtype=author&query=Nikolaidis,+C), [Damien Allonsius](https://arxiv.org/search/cs?searchtype=author&query=Allonsius,+D), [Daniel Song](https://arxiv.org/search/cs?searchtype=author&query=Song,+D), [Danielle Pintz](https://arxiv.org/search/cs?searchtype=author&query=Pintz,+D), [Danny Livshits](https://arxiv.org/search/cs?searchtype=author&query=Livshits,+D), [Danny Wyatt](https://arxiv.org/search/cs?searchtype=author&query=Wyatt,+D), [David Esiobu](https://arxiv.org/search/cs?searchtype=author&query=Esiobu,+D), [Dhruv Choudhary](https://arxiv.org/search/cs?searchtype=author&query=Choudhary,+D), [Dhruv Mahajan](https://arxiv.org/search/cs?searchtype=author&query=Mahajan,+D), [Diego Garcia-Olano](https://arxiv.org/search/cs?searchtype=author&query=Garcia-Olano,+D), [Diego Perino](https://arxiv.org/search/cs?searchtype=author&query=Perino,+D), [Dieuwke Hupkes](https://arxiv.org/search/cs?searchtype=author&query=Hupkes,+D), [Egor Lakomkin](https://arxiv.org/search/cs?searchtype=author&query=Lakomkin,+E), [Ehab AlBadawy](https://arxiv.org/search/cs?searchtype=author&query=AlBadawy,+E), [Elina Lobanova](https://arxiv.org/search/cs?searchtype=author&query=Lobanova,+E), [Emily Dinan](https://arxiv.org/search/cs?searchtype=author&query=Dinan,+E), [Eric Michael Smith](https://arxiv.org/search/cs?searchtype=author&query=Smith,+E+M), [Filip Radenovic](https://arxiv.org/search/cs?searchtype=author&query=Radenovic,+F), [Francisco Guzmán](https://arxiv.org/search/cs?searchtype=author&query=Guzm%C3%A1n,+F), [Frank Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+F), [Gabriel Synnaeve](https://arxiv.org/search/cs?searchtype=author&query=Synnaeve,+G), [Gabrielle Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+G), [Georgia Lewis Anderson](https://arxiv.org/search/cs?searchtype=author&query=Anderson,+G+L), [Govind Thattai](https://arxiv.org/search/cs?searchtype=author&query=Thattai,+G), [Graeme Nail](https://arxiv.org/search/cs?searchtype=author&query=Nail,+G), [Gregoire Mialon](https://arxiv.org/search/cs?searchtype=author&query=Mialon,+G), [Guan Pang](https://arxiv.org/search/cs?searchtype=author&query=Pang,+G), [Guillem Cucurell](https://arxiv.org/search/cs?searchtype=author&query=Cucurell,+G), [Hailey Nguyen](https://arxiv.org/search/cs?searchtype=author&query=Nguyen,+H), [Hannah Korevaar](https://arxiv.org/search/cs?searchtype=author&query=Korevaar,+H), [Hu Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+H), [Hugo Touvron](https://arxiv.org/search/cs?searchtype=author&query=Touvron,+H), [Iliyan Zarov](https://arxiv.org/search/cs?searchtype=author&query=Zarov,+I), [Imanol Arrieta Ibarra](https://arxiv.org/search/cs?searchtype=author&query=Ibarra,+I+A), [Isabel Kloumann](https://arxiv.org/search/cs?searchtype=author&query=Kloumann,+I), [Ishan Misra](https://arxiv.org/search/cs?searchtype=author&query=Misra,+I), [Ivan Evtimov](https://arxiv.org/search/cs?searchtype=author&query=Evtimov,+I), [Jack Zhang](https://arxiv.org/search/cs?searchtype=author&query=Zhang,+J), [Jade Copet](https://arxiv.org/search/cs?searchtype=author&query=Copet,+J), [Jaewon Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Jan Geffert](https://arxiv.org/search/cs?searchtype=author&query=Geffert,+J), [Jana Vranes](https://arxiv.org/search/cs?searchtype=author&query=Vranes,+J), [Jason Park](https://arxiv.org/search/cs?searchtype=author&query=Park,+J), [Jay Mahadeokar](https://arxiv.org/search/cs?searchtype=author&query=Mahadeokar,+J), [Jeet Shah](https://arxiv.org/search/cs?searchtype=author&query=Shah,+J), [Jelmer van der Linde](https://arxiv.org/search/cs?searchtype=author&query=van+der+Linde,+J), [Jennifer Billock](https://arxiv.org/search/cs?searchtype=author&query=Billock,+J), [Jenny Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong,+J), [Jenya Lee](https://arxiv.org/search/cs?searchtype=author&query=Lee,+J), [Jeremy Fu](https://arxiv.org/search/cs?searchtype=author&query=Fu,+J), [Jianfeng Chi](https://arxiv.org/search/cs?searchtype=author&query=Chi,+J), [Jianyu Huang](https://arxiv.org/search/cs?searchtype=author&query=Huang,+J), [Jiawen Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+J), [Jie Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+J), [Jiecao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+J), [Joanna Bitton](https://arxiv.org/search/cs?searchtype=author&query=Bitton,+J), [Joe Spisak](https://arxiv.org/search/cs?searchtype=author&query=Spisak,+J), [Jongsoo Park](https://arxiv.org/search/cs?searchtype=author&query=Park,+J), [Joseph Rocca](https://arxiv.org/search/cs?searchtype=author&query=Rocca,+J), [Joshua Johnstun](https://arxiv.org/search/cs?searchtype=author&query=Johnstun,+J), [Joshua Saxe](https://arxiv.org/search/cs?searchtype=author&query=Saxe,+J), [Junteng Jia](https://arxiv.org/search/cs?searchtype=author&query=Jia,+J) et al. (460 additional authors not shown)","src/content/papers/The Llama 3 Herd of Models(2024).md","755a3621c0e24361",{"html":73,"metadata":74},"\u003Ch2 id=\"arxiv\">arXiv\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/abs/2407.21783\">https://arxiv.org/abs/2407.21783\u003C/a>\u003C/p>\n\u003Ch2 id=\"要約\">要約\u003C/h2>\n\u003Cul>\n\u003Cli>Llama-3に関する研究開発\u003C/li>\n\u003Cli>15.6Tトークンで学習された8B,70B,405Bのdense Transformer\u003C/li>\n\u003Cli>128Kトークンのコンテクストウィンドウ\u003C/li>\n\u003Cli>\u003Cstrong>DPO！！！\u003C/strong>\u003C/li>\n\u003Cli>4oとかClaude3.5とかに匹敵するレベル\u003C/li>\n\u003Cli>Llama-2 70BとLlama-3 8Bが同じぐらいの性能\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"学習方法\">学習方法\u003C/h2>\n\u003Col>\n\u003Cli>教師なし学習により、大量のテキストコーパスから学習\u003C/li>\n\u003Cli>現在のモデルでプリファレンスデータ（出力を人手で比較評価したやつ）を作成し、報酬モデリングを実施\u003C/li>\n\u003Cli>現在のモデル＋報酬モデルSFT用のデータを作成し、ベースモデルをチューニング\u003C/li>\n\u003Cli>プリファレンスデータをLLMにマッチさせるようにDPOでベストモデルを更新\u003C/li>\n\u003C/ol>\n\u003Ch2 id=\"開発における3つの主要要素\">開発における3つの主要要素\u003C/h2>\n\u003Ch3 id=\"データ\">データ\u003C/h3>\n\u003Cul>\n\u003Cli>事前、事後両方の学習でのデータの質と量が向上\u003C/li>\n\u003Cli>データミックスは、一般知識50％、数学的推論25％、コード17％、多言語トークン8％\u003C/li>\n\u003Cli>安全性のためのデータフィルタリング\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"スケール\">スケール\u003C/h3>\n\u003Cul>\n\u003Cli>Llama 2の最大verより約50倍の計算量でモデルを訓練\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"複雑性の管理\">複雑性の管理\u003C/h3>\n\u003Cul>\n\u003Cli>標準のdense Transformerのアーキテクチャを採用\u003C/li>\n\u003Cli>MoEは複雑だから避けて安定性重視\n\u003Cul>\n\u003Cli>MoEってなんですかの人はこちら\u003C/li>\n\u003Cli>\u003Ca href=\"https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\">https://www.ibm.com/jp-ja/think/topics/mixture-of-experts\u003C/a>\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"llama-2からの変更点\">Llama 2からの変更点\u003C/h2>\n\u003Ch3 id=\"ppo--dpo\">PPO → DPO\u003C/h3>\n\u003Cul>\n\u003Cli>ここで出てきますねDPO\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"データ量\">データ量\u003C/h3>\n\u003Cul>\n\u003Cli>1.8兆tokens → 15兆tokens\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"モデルアーキテクチャの微変更\">モデルアーキテクチャの微変更\u003C/h3>\n\u003Cul>\n\u003Cli>Grouped Query Attention(GQA)の採用\n\u003Cul>\n\u003Cli>GQAのGemini解説: \u003Ca href=\"https://g.co/gemini/share/5a3c1c47f73d\">https://g.co/gemini/share/5a3c1c47f73d\u003C/a>\u003C/li>\n\u003Cli>推論速度向上\u003C/li>\n\u003Cli>デコード中のキャッシュサイズ削減\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>トークンボキャブラリの拡張\n\u003Cul>\n\u003Cli>tiktokenトークナイザの100K+非英語言語用の28Kトークン\u003C/li>\n\u003Cli>英語データの圧縮率が3.17文字/token から3.94文字/token に向上\n\u003Cul>\n\u003Cli>同じ計算量でいっぱい読める\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>RoPEベースの周波数ハイパラが増加\n\u003Cul>\n\u003Cli>RoPEは位置情報を位置の加算じゃなくて回転するやつ\u003C/li>\n\u003Cli>500,000に増加(何が嬉しいのかよくわからなかった)\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"感想\">感想\u003C/h2>\n\u003Cul>\n\u003Cli>もうDPOしか頭に入ってこなかった\u003C/li>\n\u003Cli>RoPEのところの話があんまり理解できなかった\u003C/li>\n\u003Cli>やっぱりLlamaは安全性にすごい気を使っていそう\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"著者多すぎ\">著者(多すぎ)\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Grattafiori,+A\">Aaron Grattafiori\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Dubey,+A\">Abhimanyu Dubey\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Jauhri,+A\">Abhinav Jauhri\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pandey,+A\">Abhinav Pandey\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Kadian,+A\">Abhishek Kadian\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Al-Dahle,+A\">Ahmad Al-Dahle\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Letman,+A\">Aiesha Letman\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mathur,+A\">Akhil Mathur\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Schelten,+A\">Alan Schelten\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Vaughan,+A\">Alex Vaughan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yang,+A\">Amy Yang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Fan,+A\">Angela Fan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Goyal,+A\">Anirudh Goyal\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hartshorn,+A\">Anthony Hartshorn\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yang,+A\">Aobo Yang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mitra,+A\">Archi Mitra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Archie\">Archie Sravankumar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Korenev,+A\">Artem Korenev\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hinsvark,+A\">Arthur Hinsvark\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rao,+A\">Arun Rao\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+A\">Aston Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rodriguez,+A\">Aurelien Rodriguez\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Gregerson,+A\">Austen Gregerson\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Spataru,+A\">Ava Spataru\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Roziere,+B\">Baptiste Roziere\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Biron,+B\">Bethany Biron\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Tang,+B\">Binh Tang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chern,+B\">Bobbie Chern\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Caucheteux,+C\">Charlotte Caucheteux\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nayak,+C\">Chaya Nayak\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Bi,+C\">Chloe Bi\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Marra,+C\">Chris Marra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=McConnell,+C\">Chris McConnell\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Keller,+C\">Christian Keller\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Touret,+C\">Christophe Touret\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wu,+C\">Chunyang Wu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wong,+C\">Corinne Wong\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ferrer,+C+C\">Cristian Canton Ferrer\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nikolaidis,+C\">Cyrus Nikolaidis\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Allonsius,+D\">Damien Allonsius\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Song,+D\">Daniel Song\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pintz,+D\">Danielle Pintz\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Livshits,+D\">Danny Livshits\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wyatt,+D\">Danny Wyatt\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Esiobu,+D\">David Esiobu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Choudhary,+D\">Dhruv Choudhary\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mahajan,+D\">Dhruv Mahajan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Garcia-Olano,+D\">Diego Garcia-Olano\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Perino,+D\">Diego Perino\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hupkes,+D\">Dieuwke Hupkes\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lakomkin,+E\">Egor Lakomkin\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=AlBadawy,+E\">Ehab AlBadawy\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lobanova,+E\">Elina Lobanova\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Dinan,+E\">Emily Dinan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Smith,+E+M\">Eric Michael Smith\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Radenovic,+F\">Filip Radenovic\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Guzm%C3%A1n,+F\">Francisco Guzmán\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+F\">Frank Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Synnaeve,+G\">Gabriel Synnaeve\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+G\">Gabrielle Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Anderson,+G+L\">Georgia Lewis Anderson\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Thattai,+G\">Govind Thattai\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nail,+G\">Graeme Nail\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mialon,+G\">Gregoire Mialon\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pang,+G\">Guan Pang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Cucurell,+G\">Guillem Cucurell\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Nguyen,+H\">Hailey Nguyen\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Korevaar,+H\">Hannah Korevaar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Xu,+H\">Hu Xu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Touvron,+H\">Hugo Touvron\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zarov,+I\">Iliyan Zarov\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ibarra,+I+A\">Imanol Arrieta Ibarra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Kloumann,+I\">Isabel Kloumann\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Misra,+I\">Ishan Misra\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Evtimov,+I\">Ivan Evtimov\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhang,+J\">Jack Zhang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Copet,+J\">Jade Copet\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+J\">Jaewon Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Geffert,+J\">Jan Geffert\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Vranes,+J\">Jana Vranes\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Park,+J\">Jason Park\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Mahadeokar,+J\">Jay Mahadeokar\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Shah,+J\">Jeet Shah\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=van+der+Linde,+J\">Jelmer van der Linde\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Billock,+J\">Jennifer Billock\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Hong,+J\">Jenny Hong\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Lee,+J\">Jenya Lee\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Fu,+J\">Jeremy Fu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chi,+J\">Jianfeng Chi\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Huang,+J\">Jianyu Huang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Liu,+J\">Jiawen Liu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Wang,+J\">Jie Wang\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Yu,+J\">Jiecao Yu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Bitton,+J\">Joanna Bitton\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Spisak,+J\">Joe Spisak\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Park,+J\">Jongsoo Park\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Rocca,+J\">Joseph Rocca\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Johnstun,+J\">Joshua Johnstun\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Saxe,+J\">Joshua Saxe\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Jia,+J\">Junteng Jia\u003C/a> et al. (460 additional authors not shown)\u003C/p>",{"headings":75,"localImagePaths":102,"remoteImagePaths":103,"frontmatter":104,"imagePaths":105},[76,77,78,80,82,84,86,88,91,94,96,98,99],{"depth":24,"slug":25,"text":26},{"depth":24,"slug":30,"text":30},{"depth":24,"slug":79,"text":79},"学習方法",{"depth":24,"slug":81,"text":81},"開発における3つの主要要素",{"depth":34,"slug":83,"text":83},"データ",{"depth":34,"slug":85,"text":85},"スケール",{"depth":34,"slug":87,"text":87},"複雑性の管理",{"depth":24,"slug":89,"text":90},"llama-2からの変更点","Llama 2からの変更点",{"depth":34,"slug":92,"text":93},"ppo--dpo","PPO → DPO",{"depth":34,"slug":95,"text":95},"データ量",{"depth":34,"slug":97,"text":97},"モデルアーキテクチャの微変更",{"depth":24,"slug":58,"text":58},{"depth":24,"slug":100,"text":101},"著者多すぎ","著者(多すぎ)",[],[],{"title":67,"date":68},[],"The Llama 3 Herd of Models(2024).md","large-language-models-are-human-level-prompt-engineers2023",{"id":107,"data":109,"body":112,"filePath":113,"digest":114,"rendered":115,"legacyId":165},{"title":110,"date":111},"Large Language Models Are Human-Level Prompt Engineers (2023)","2025-06-06","## 著者\n[Yongchao Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+Y), [Andrei Ioan Muresanu](https://arxiv.org/search/cs?searchtype=author&query=Muresanu,+A+I), [Ziwen Han](https://arxiv.org/search/cs?searchtype=author&query=Han,+Z), [Keiran Paster](https://arxiv.org/search/cs?searchtype=author&query=Paster,+K), [Silviu Pitis](https://arxiv.org/search/cs?searchtype=author&query=Pitis,+S), [Harris Chan](https://arxiv.org/search/cs?searchtype=author&query=Chan,+H), [Jimmy Ba](https://arxiv.org/search/cs?searchtype=author&query=Ba,+J)\n\n## arXivリンク\nhttps://arxiv.org/abs/2211.01910\n\n## 要約\n- プロンプトエンジニアリングを自動化する「Automatic Prompt Engineer (APE)」を提案\n-  ざっくり**LLMにLLMのプロンプトを考えさせる**手法。\n- APEは人間が時間をかけて考えたものと同等かそれ以上の性能を達成\n-  APEのステップ\n\t- LLMに「こういう入力をしたらこういう出力をする」という例を見せて、指示文の候補をいっぱい作らせる\n\t- 候補を試し、その結果をスコアリング\n\t- 最も高いスコアの指示を最良のプロンプトとする\n\n## Abstract\n- LLMは汎用的でいいけど「人間が望むことをさせる」のって難しいよね\n\t- →プロンプト大事\n- 幅広いプロンプトを試す必要があるけどどの指示がどのモデルでいいのかわからない\n- LLMを自然言語でプログラムが指定されるブラックボックスなコンピュータとみなそう\n### 提案手法(APE)\nゼロショット学習において24件中24件のInstruction Inductionタスクと21件中17件のBig-Benchタスクで人間レベルの性能を達成\n\n## APEのアルゴリズム(お気持ちベース)\n### \"提案→評価とフィルタリング→選択\"のサイクルを自動化\n### 指示候補の提案\n- LLMにタスクの入出力例を見せ、タスクの遂行用指示文をいっぱい考えさせる。\n- 順方向生成\n\t- 例を先に提示し、最後にこういう指示でしたと続ける\n- 逆方向生成\n\t- \u003Cここに指示を挿入>的な感じで穴埋め問題にして、その後に例を提示\n### 評価・フィルタリング\n指示候補を使って、実際に別のLLMにタスクを解かせ、その性能をスコア付け\n#### 評価基準\n- **実行精度 (Execution accuracy)**: 指示通りにタスクを実行した結果が、想定される正解と一致したかどうかで評価\n- **対数尤度 (Log probability)**: どれだけ正解に近い答えを生成できそうかを確率で評価\n#### フィルタリング\n- 少数の訓練データで候補を評価\n- スコアが良かった上位数パーを残して破棄\n- 残ったものを別の訓練データで再評価\n- これを繰り返して少数の交互に絞る\n#### 選択\n- フィルタリングで残った候補から最も高いスコアのものを採用\n\n## APEを使った結果\n-  ゼロショットで24個のInstruction Inductionタスク全てにおいて、人間が作成したプロンプトと同等かそれ以上の性能を達成\n-  フューショットで24タスク中21タスクで性能が向上するか、同等の結果\n- 高難易度タスク(BIG-Bench)でも17/21で同等かそれ以上\n## いい感じの発見\n### Zero-shot Chain-of-Thought\n- \"Let's think step by step.\"をAPEが改善\n\t- \"Let's work this out in a step by step way to be sure we have the right answer\"のほうがいいらしい\n### TruthfulQA\n- APEがLLMの応答スタイルを制御して「真実性」と「情報提供性」のトレードオフを発見\n\t- 真実性をあげるために嘘をつかせないプロンプト(you have no comment→回答を拒否する選択肢をあたえる)をAPEが見つけた\n\n## 結論\n人間による入力を最小限にしつつ、最適なプロンプトをみつける方法としてAPEは有用\n\n## 感想\n- プロンプト集みたいなのがよくネットに転がってるけどこういうのでちゃんと性能が確認されているのか気になった。\n- 人間が直感的にわかりやすいプロンプトと、LLMがいい性能を示すプロンプトがちょっとちがっておもろい\n- モデル間でも最適なプロンプトがちがって、InstractGPTで最適なプロンプトをGPT-3で用いるとスコアが下がることがあるらしい←自分が使ってるモデルで最適なのを調べる必要あり\n- ↑モデルごと違うならそれこそ自動化とかしないと見つけるの大変そう","src/content/papers/Large Language Models Are Human-Level Prompt Engineers(2023).md","75a5091659b958d9",{"html":116,"metadata":117},"\u003Ch2 id=\"著者\">著者\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Zhou,+Y\">Yongchao Zhou\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Muresanu,+A+I\">Andrei Ioan Muresanu\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Han,+Z\">Ziwen Han\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Paster,+K\">Keiran Paster\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Pitis,+S\">Silviu Pitis\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Chan,+H\">Harris Chan\u003C/a>, \u003Ca href=\"https://arxiv.org/search/cs?searchtype=author&#x26;query=Ba,+J\">Jimmy Ba\u003C/a>\u003C/p>\n\u003Ch2 id=\"arxivリンク\">arXivリンク\u003C/h2>\n\u003Cp>\u003Ca href=\"https://arxiv.org/abs/2211.01910\">https://arxiv.org/abs/2211.01910\u003C/a>\u003C/p>\n\u003Ch2 id=\"要約\">要約\u003C/h2>\n\u003Cul>\n\u003Cli>プロンプトエンジニアリングを自動化する「Automatic Prompt Engineer (APE)」を提案\u003C/li>\n\u003Cli>ざっくり\u003Cstrong>LLMにLLMのプロンプトを考えさせる\u003C/strong>手法。\u003C/li>\n\u003Cli>APEは人間が時間をかけて考えたものと同等かそれ以上の性能を達成\u003C/li>\n\u003Cli>APEのステップ\n\u003Cul>\n\u003Cli>LLMに「こういう入力をしたらこういう出力をする」という例を見せて、指示文の候補をいっぱい作らせる\u003C/li>\n\u003Cli>候補を試し、その結果をスコアリング\u003C/li>\n\u003Cli>最も高いスコアの指示を最良のプロンプトとする\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"abstract\">Abstract\u003C/h2>\n\u003Cul>\n\u003Cli>LLMは汎用的でいいけど「人間が望むことをさせる」のって難しいよね\n\u003Cul>\n\u003Cli>→プロンプト大事\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>幅広いプロンプトを試す必要があるけどどの指示がどのモデルでいいのかわからない\u003C/li>\n\u003Cli>LLMを自然言語でプログラムが指定されるブラックボックスなコンピュータとみなそう\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"提案手法ape\">提案手法(APE)\u003C/h3>\n\u003Cp>ゼロショット学習において24件中24件のInstruction Inductionタスクと21件中17件のBig-Benchタスクで人間レベルの性能を達成\u003C/p>\n\u003Ch2 id=\"apeのアルゴリズムお気持ちベース\">APEのアルゴリズム(お気持ちベース)\u003C/h2>\n\u003Ch3 id=\"提案評価とフィルタリング選択のサイクルを自動化\">“提案→評価とフィルタリング→選択”のサイクルを自動化\u003C/h3>\n\u003Ch3 id=\"指示候補の提案\">指示候補の提案\u003C/h3>\n\u003Cul>\n\u003Cli>LLMにタスクの入出力例を見せ、タスクの遂行用指示文をいっぱい考えさせる。\u003C/li>\n\u003Cli>順方向生成\n\u003Cul>\n\u003Cli>例を先に提示し、最後にこういう指示でしたと続ける\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003Cli>逆方向生成\n\u003Cul>\n\u003Cli>&#x3C;ここに指示を挿入>的な感じで穴埋め問題にして、その後に例を提示\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"評価フィルタリング\">評価・フィルタリング\u003C/h3>\n\u003Cp>指示候補を使って、実際に別のLLMにタスクを解かせ、その性能をスコア付け\u003C/p>\n\u003Ch4 id=\"評価基準\">評価基準\u003C/h4>\n\u003Cul>\n\u003Cli>\u003Cstrong>実行精度 (Execution accuracy)\u003C/strong>: 指示通りにタスクを実行した結果が、想定される正解と一致したかどうかで評価\u003C/li>\n\u003Cli>\u003Cstrong>対数尤度 (Log probability)\u003C/strong>: どれだけ正解に近い答えを生成できそうかを確率で評価\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"フィルタリング\">フィルタリング\u003C/h4>\n\u003Cul>\n\u003Cli>少数の訓練データで候補を評価\u003C/li>\n\u003Cli>スコアが良かった上位数パーを残して破棄\u003C/li>\n\u003Cli>残ったものを別の訓練データで再評価\u003C/li>\n\u003Cli>これを繰り返して少数の交互に絞る\u003C/li>\n\u003C/ul>\n\u003Ch4 id=\"選択\">選択\u003C/h4>\n\u003Cul>\n\u003Cli>フィルタリングで残った候補から最も高いスコアのものを採用\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"apeを使った結果\">APEを使った結果\u003C/h2>\n\u003Cul>\n\u003Cli>ゼロショットで24個のInstruction Inductionタスク全てにおいて、人間が作成したプロンプトと同等かそれ以上の性能を達成\u003C/li>\n\u003Cli>フューショットで24タスク中21タスクで性能が向上するか、同等の結果\u003C/li>\n\u003Cli>高難易度タスク(BIG-Bench)でも17/21で同等かそれ以上\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"いい感じの発見\">いい感じの発見\u003C/h2>\n\u003Ch3 id=\"zero-shot-chain-of-thought\">Zero-shot Chain-of-Thought\u003C/h3>\n\u003Cul>\n\u003Cli>“Let’s think step by step.”をAPEが改善\n\u003Cul>\n\u003Cli>“Let’s work this out in a step by step way to be sure we have the right answer”のほうがいいらしい\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch3 id=\"truthfulqa\">TruthfulQA\u003C/h3>\n\u003Cul>\n\u003Cli>APEがLLMの応答スタイルを制御して「真実性」と「情報提供性」のトレードオフを発見\n\u003Cul>\n\u003Cli>真実性をあげるために嘘をつかせないプロンプト(you have no comment→回答を拒否する選択肢をあたえる)をAPEが見つけた\u003C/li>\n\u003C/ul>\n\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"結論\">結論\u003C/h2>\n\u003Cp>人間による入力を最小限にしつつ、最適なプロンプトをみつける方法としてAPEは有用\u003C/p>\n\u003Ch2 id=\"感想\">感想\u003C/h2>\n\u003Cul>\n\u003Cli>プロンプト集みたいなのがよくネットに転がってるけどこういうのでちゃんと性能が確認されているのか気になった。\u003C/li>\n\u003Cli>人間が直感的にわかりやすいプロンプトと、LLMがいい性能を示すプロンプトがちょっとちがっておもろい\u003C/li>\n\u003Cli>モデル間でも最適なプロンプトがちがって、InstractGPTで最適なプロンプトをGPT-3で用いるとスコアが下がることがあるらしい←自分が使ってるモデルで最適なのを調べる必要あり\u003C/li>\n\u003Cli>↑モデルごと違うならそれこそ自動化とかしないと見つけるの大変そう\u003C/li>\n\u003C/ul>",{"headings":118,"localImagePaths":161,"remoteImagePaths":162,"frontmatter":163,"imagePaths":164},[119,120,123,124,127,130,133,136,138,141,144,146,148,151,153,156,159,160],{"depth":24,"slug":28,"text":28},{"depth":24,"slug":121,"text":122},"arxivリンク","arXivリンク",{"depth":24,"slug":30,"text":30},{"depth":24,"slug":125,"text":126},"abstract","Abstract",{"depth":34,"slug":128,"text":129},"提案手法ape","提案手法(APE)",{"depth":24,"slug":131,"text":132},"apeのアルゴリズムお気持ちベース","APEのアルゴリズム(お気持ちベース)",{"depth":34,"slug":134,"text":135},"提案評価とフィルタリング選択のサイクルを自動化","“提案→評価とフィルタリング→選択”のサイクルを自動化",{"depth":34,"slug":137,"text":137},"指示候補の提案",{"depth":34,"slug":139,"text":140},"評価フィルタリング","評価・フィルタリング",{"depth":142,"slug":143,"text":143},4,"評価基準",{"depth":142,"slug":145,"text":145},"フィルタリング",{"depth":142,"slug":147,"text":147},"選択",{"depth":24,"slug":149,"text":150},"apeを使った結果","APEを使った結果",{"depth":24,"slug":152,"text":152},"いい感じの発見",{"depth":34,"slug":154,"text":155},"zero-shot-chain-of-thought","Zero-shot Chain-of-Thought",{"depth":34,"slug":157,"text":158},"truthfulqa","TruthfulQA",{"depth":24,"slug":52,"text":52},{"depth":24,"slug":58,"text":58},[],[],{"title":110,"date":111},[],"Large Language Models Are Human-Level Prompt Engineers(2023).md"]